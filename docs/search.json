[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "",
    "text": "Workshop on Sample Size Planning for  Intensive Longitudinal Studies\nGinette Lafit, Jordan Revol, Mihai A. Constantin, & Eva Ceulemans"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìù Description",
    "text": "üìù Description\nIn recent years the popularity of procedures to collect intensive longitudinal data such as the Experience Sampling Method has increased immensely. The data collected using such designs allow researchers to study the dynamics of psychological processes, and how these dynamics differ across individuals. A fundamental question when designing a study is how to determine the sample size, which is closely related to the replicability and generalizability of empirical findings. Even though multiple statistical guidelines are available for sample size planning, it still remains a demanding enterprise in complex designs. The goal of this workshop is to address this crucial question by presenting methodological advances for sample size planning for intensive longitudinal designs. First, we provide an overview of methods for sample size planning with special emphasis on a priori power analysis. Second, we focus on how to conduct power analysis in the \\(N = 1\\) case when the goal is to model within-person processes using \\(\\text{VAR}(1)\\) models. Subsequently, we consider the extension to multilevel data in which multiple individuals are measured over time. We introduce an approach for conducting power analysis for multilevel models that explicitly accounts for the temporal dependencies that characterize the data collected in intensive longitudinal studies. In addition, we showcase how to perform power analysis for these models using a user-friendly and open-source application. Finally, we consider an alternative criterion for conducting sample size planning that targets the predictive accuracy of a model for unseen data. Focusing on \\(\\text{VAR}(1)\\) models in an \\(N = 1\\) context, we introduce a novel approach, called predictive accuracy analysis, to assess how many measurement occasions are required in order to optimize predictive accuracy."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìä Learning Objectives",
    "text": "üìä Learning Objectives\nThe workshop provides a road map on how to determine the sample size in intensive longitudinal designs. Upon course completion, participants will:\n\nbe familiar with methods for conducting power analysis for \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models in \\(N = 1\\) and multilevel intensive longitudinal designs\nunderstand the key differences between simulation-based and analytical power analysis approaches\nbe able to leverage existing tools for conducting power analysis for \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) for intensive longitudinal designs\nbe familiar with new methods for conducting sample size analysis based on criteria different than statistical power (e.g., predictive accuracy or sensitivity)"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üíª Prerequisites",
    "text": "üíª Prerequisites\nParticipants should have some basic knowledge of R and some experience with RStudio. For the hands-on parts of the workshop, you need to install R version 4.1.2 or higher, RStudio, and several R packages as indicated on the page corresponding to each exercise.\nSome exercises in this workshop also involve using Shiny applications to run power analysis. You can find additional instructions on how to download and run the Shiny applications below:\n\nfor Predictive Accuracy Analysis and power analysis for the \\(\\text{VAR}(1)\\)\nfor power analysis for multilevel models using the simulation-based approach\nfor power analysis for multilevel models using the analytic approach\n\nYou can find detailed instructions and examples for conducting sample size analysis using the powerly package at powerly.dev."
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìÇ Modules",
    "text": "üìÇ Modules\n\n\n\nTopic\nDuration\nSlides\nTutorial\n\n\n\n\nIntroduction to sample size planning in intensive longitudinal research\n45m\nslides\n-\n\n\nSample size planning for \\(\\text{VAR}(1)\\) models in \\(N = 1\\) designs\n60m\nslides\ntutorial 1 tutorial 2\n\n\nSample size planning for multilevel models applied to intensive longitudinal designs\n50m\nslides\ntutorial 1 tutorial 2 tutorial 3\n\n\nAdvanced methods for sample size analysis\n40m\nslides\ntutorial"
  },
  {
    "objectID": "index.html#given-at",
    "href": "index.html#given-at",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìç Given At",
    "text": "üìç Given At\n\n\n\nConference\nLocation\nDate\nLink\n\n\n\n\nSAA 2023\nAmsterdam, The Netherlands\nJune 8th, 2023\nlink"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "‚úçÔ∏è Citation",
    "text": "‚úçÔ∏è Citation\n\nLafit, G., Revol, J., Constantin M. A., & Ceulemans, E. (2023). Workshop on Sample Size Planning for Intensive Longitudinal Studies. https://00.0000/zenodo.0000000"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "‚öñÔ∏è License",
    "text": "‚öñÔ∏è License\n\n\nThe scripts, slides, and other materials by Ginette Lafit, Jordan Revol, Mihai A. Constantin, and Eva Ceulemans are licensed under CC BY 4.0  ."
  },
  {
    "objectID": "presenters/eva-ceulemans.html",
    "href": "presenters/eva-ceulemans.html",
    "title": "Eva Ceulemans",
    "section": "",
    "text": "Professor of Quantitative Psychology and Individual Differences at KU Leuven"
  },
  {
    "objectID": "presenters/eva-ceulemans.html#interests",
    "href": "presenters/eva-ceulemans.html#interests",
    "title": "Eva Ceulemans",
    "section": "üîç Interests",
    "text": "üîç Interests\nDimension reduction methods for multiset and multiway data\nTime series analysis\nChange point detection methods\nAnalysis of dyadic data\nModeling of emotion dynamics"
  },
  {
    "objectID": "presenters/ginette-lafit.html",
    "href": "presenters/ginette-lafit.html",
    "title": "Ginette Lafit",
    "section": "",
    "text": "PostDoc at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences and Center of Contextual Psychiatry"
  },
  {
    "objectID": "presenters/ginette-lafit.html#section",
    "href": "presenters/ginette-lafit.html#section",
    "title": "Ginette Lafit",
    "section": "",
    "text": "PostDoc at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences and Center of Contextual Psychiatry"
  },
  {
    "objectID": "presenters/ginette-lafit.html#interests",
    "href": "presenters/ginette-lafit.html#interests",
    "title": "Ginette Lafit",
    "section": "üîç Interests",
    "text": "üîç Interests\nStatistical methods for intensive longitudinal designs\nMethodology & open science in ESM research\nModel selection and regularization\nDimension reduction models"
  },
  {
    "objectID": "presenters/jordan-revol.html",
    "href": "presenters/jordan-revol.html",
    "title": "Jordan Revol",
    "section": "",
    "text": "PhD candidate at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences"
  },
  {
    "objectID": "presenters/jordan-revol.html#section",
    "href": "presenters/jordan-revol.html#section",
    "title": "Jordan Revol",
    "section": "",
    "text": "PhD candidate at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences"
  },
  {
    "objectID": "presenters/jordan-revol.html#interests",
    "href": "presenters/jordan-revol.html#interests",
    "title": "Jordan Revol",
    "section": "üîç Interests",
    "text": "üîç Interests\nStatistical methods for intensive longitudinal designs\nSample size planning methods and predictive accuracy in intensive longitudinal studies"
  },
  {
    "objectID": "presenters/mihai-constantin.html",
    "href": "presenters/mihai-constantin.html",
    "title": "Mihai Constantin",
    "section": "",
    "text": "PhD candidate at Tilburg University in the Department of Methodology and Statistics"
  },
  {
    "objectID": "presenters/mihai-constantin.html#section",
    "href": "presenters/mihai-constantin.html#section",
    "title": "Mihai Constantin",
    "section": "",
    "text": "PhD candidate at Tilburg University in the Department of Methodology and Statistics"
  },
  {
    "objectID": "presenters/mihai-constantin.html#interests",
    "href": "presenters/mihai-constantin.html#interests",
    "title": "Mihai Constantin",
    "section": "üîç Interests",
    "text": "üîç Interests\nMingling psychology with data science and software engineering\nComputational statistics and developing software tools for social sciences\nSimulation-based methods for sample size analysis"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ntidyverse\ndata.table\npsych\nviridis\ndevtools\nnlme\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\n    \"tidyverse\", \"data.table\", \"psych\",\n    \"viridis\", \"devtools\", \"nlme\"\n)\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nNow that we have all packages installed, we continue by loading them.\n\n# Handy collection of packages for data manipulation and plotting.\nlibrary(tidyverse)\n\n# To create lagged outcome.\nlibrary(data.table)\n\n# To compute descriptive statistics.\nlibrary(psych)\n\n# Color scales adapted for colorblindness.\nlibrary(viridis)\n\n# To estimate mixed-effects models.\nlibrary(nlme)\n\nAdditionally, we may also need to install and load the PowerLAPIM package from GitHub:\n\n# Install\ndevtools::install_github(\"ginettelafit/PowerLAPIM\", force = TRUE)\n\nTo complete the setup, we also need to set the seed for reproducibility.\n\n# Set a seed for reproducibility.\nset.seed(123)"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html#settings-things-up",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html#settings-things-up",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ntidyverse\ndata.table\npsych\nviridis\ndevtools\nnlme\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\n    \"tidyverse\", \"data.table\", \"psych\",\n    \"viridis\", \"devtools\", \"nlme\"\n)\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nNow that we have all packages installed, we continue by loading them.\n\n# Handy collection of packages for data manipulation and plotting.\nlibrary(tidyverse)\n\n# To create lagged outcome.\nlibrary(data.table)\n\n# To compute descriptive statistics.\nlibrary(psych)\n\n# Color scales adapted for colorblindness.\nlibrary(viridis)\n\n# To estimate mixed-effects models.\nlibrary(nlme)\n\nAdditionally, we may also need to install and load the PowerLAPIM package from GitHub:\n\n# Install\ndevtools::install_github(\"ginettelafit/PowerLAPIM\", force = TRUE)\n\nTo complete the setup, we also need to set the seed for reproducibility.\n\n# Set a seed for reproducibility.\nset.seed(123)"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html#description",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html#description",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "Description",
    "text": "Description\nIn this tutorial, we use data from Heininga et al. (2019). In this study, the authors applied the ESM methodology to study emotion dynamics in people with Major Depressive Disorder (MDD). The study consist of an ESM testing period of seven days in which participants had to fill out questions about mood and social context on their daily lives ten times a day (i.e., \\(70\\) measurement occasions). The data set contains \\(38\\) participants diagnosed with MDD and \\(40\\) control subjects. Participants filled out the ESM questionnaires in a stratified random interval scheme between 9:30 AM and 9:30 PM.\nThe data set contains the following variables:\n\nPID that denotes the individual identification number\nday is a variable that ranges from 1 to 7 and identifies the day of ESM testing\ndaybeep is a variable that ranges from 1 to 10 and identifies the number of the prompt or beep within a day\nPA is the positive affect computed as the mean of items:\n\nHow happy do you feel at the moment?\nHow relaxed do you feel at the moment?\nHow euphoric do you feel at the moment?\n\nNA. is the negative affect computed as the mean of items:\n\nHow depressed do you feel at the moment?\nHow stressed do you feel at the moment?\nHow anxious do you feel at the moment?\nHow angry do you feel at the moment?\nHow restless do you feel at the moment?\n\nanhedonia corresponds to the ESM item:\n\nTo what degree do you find it difficult to experience pleasure in activities at the moment?\n\nMDD is a dummy variable equal to one when the individual has been diagnosed with MDD and 0 otherwise\nQIDS denotes the sum of the items of the Quick Inventory of Depressive Symptomatology (QIDS; Rush et al., 2003). QIDS was measured before the ESM testing period.\n\nFirst, we are going to load the data set:\n\n# Load the data set.\nload(file = \"assets/data/clinical-dataset.RData\")\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure you load the data from the location where you downloaded it. If your analysis script (i.e., the .R file) and the dataset are in the same location, than you can simply load the data as follows:\nload(file = \"clinical-dataset.RData\")"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html#data-exploration",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html#data-exploration",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "Data exploration",
    "text": "Data exploration\nIn this section we will explore briefly the variables in the data set.\n\nData structure\nNow, that we have the data ready, we can start by exploring it to get a better understanding of the variable measured.\n\n# Find the dimensions.\ndim(data)\n\n[1] 5460    8\n\n# Find the structure.\nstr(data)\n\n'data.frame':   5460 obs. of  8 variables:\n $ PID      : num  101 101 101 101 101 101 101 101 101 101 ...\n $ day      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ daybeep  : num  1 2 3 4 5 6 7 8 9 10 ...\n $ PA       : num  NA 27.3 49.7 43 43 ...\n $ NA.      : num  NA 30.4 23.8 24.2 32.8 19.6 18.4 21.2 23 21.8 ...\n $ anhedonia: num  NA 26 25 25 50 21 42 30 22 30 ...\n $ MDD      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ QIDS     : num  12 12 12 12 12 12 12 12 12 12 ...\n\n# See the first 6 rows.\nhead(data)\n\n  PID day daybeep       PA  NA. anhedonia MDD QIDS\n1 101   1       1       NA   NA        NA   1   12\n2 101   1       2 27.33333 30.4        26   1   12\n3 101   1       3 49.66667 23.8        25   1   12\n4 101   1       4 43.00000 24.2        25   1   12\n5 101   1       5 43.00000 32.8        50   1   12\n6 101   1       6 18.00000 19.6        21   1   12\n\n# See the last 6 rows.\ntail(data)\n\n     PID day daybeep       PA  NA. anhedonia MDD QIDS\n5455 645   7       5 70.66667  9.4        10   0    4\n5456 645   7       6 73.66667 11.0        20   0    4\n5457 645   7       7 64.33333 10.8        18   0    4\n5458 645   7       8 69.66667 11.2        10   0    4\n5459 645   7       9 73.33333 13.0        18   0    4\n5460 645   7      10 65.66667 15.2        15   0    4\n\n# Find the column names.\nnames(data)\n\n[1] \"PID\"       \"day\"       \"daybeep\"   \"PA\"        \"NA.\"       \"anhedonia\"\n[7] \"MDD\"       \"QIDS\"     \n\n# Summary of the data.\nsummary(data)\n\n      PID             day       daybeep           PA             NA.        \n Min.   :101.0   Min.   :1   Min.   : 1.0   Min.   : 0.00   Min.   :  0.00  \n 1st Qu.:131.0   1st Qu.:2   1st Qu.: 3.0   1st Qu.:23.00   1st Qu.:  6.60  \n Median :601.5   Median :4   Median : 5.5   Median :36.33   Median : 18.80  \n Mean   :390.2   Mean   :4   Mean   : 5.5   Mean   :37.07   Mean   : 25.61  \n 3rd Qu.:624.0   3rd Qu.:6   3rd Qu.: 8.0   3rd Qu.:50.00   3rd Qu.: 40.30  \n Max.   :645.0   Max.   :7   Max.   :10.0   Max.   :93.67   Max.   :100.00  \n                                            NA's   :629     NA's   :629     \n   anhedonia           MDD              QIDS       \n Min.   :  0.00   Min.   :0.0000   Min.   : 0.000  \n 1st Qu.:  7.00   1st Qu.:0.0000   1st Qu.: 3.000  \n Median : 28.00   Median :0.0000   Median : 8.000  \n Mean   : 33.34   Mean   :0.4872   Mean   : 9.359  \n 3rd Qu.: 56.00   3rd Qu.:1.0000   3rd Qu.:16.000  \n Max.   :100.00   Max.   :1.0000   Max.   :24.000  \n NA's   :629                                       \n\n# Number of participants.\nlength(unique(data$PID))\n\n[1] 78\n\n# Create variable to store the number of observations per person.\ndata$obs = rep(0, nrow(data))\n\n# Count the number of observation per person.\nfor (i in unique(data$PID)) {\n    data$obs[which(data$PID == i)] &lt;- 1:length(which(data$PID == i))\n}\n\n# Show the number of observations per person.\ntable(data$obs)\n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 \n78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 \n\n\n\n\nDescriptive statistics and visualizations\nWe first compute descriptive statistics including number of participant, number of observations per day, and compliance.\n\n# Get number of participants.\nlength(unique(data$PID))\n\n[1] 78\n\n# Obtain number of participants diagnosed with `MDD`.\nlength(unique(data$PID[data$MDD == 1]))\n\n[1] 38\n\n# Obtain number of participants in the control group.\nlength(unique(data$PID[data$MDD == 0]))\n\n[1] 40\n\n# Get the number of assessment per day.\ntable(data$PID)\n\n\n101 102 103 105 108 109 110 111 112 114 115 117 119 120 121 124 125 128 130 131 \n 70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70 \n132 135 136 138 139 140 141 143 144 202 204 205 206 207 208 210 211 306 601 602 \n 70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70 \n603 604 606 607 608 609 610 611 612 613 614 615 616 618 619 621 622 623 624 625 \n 70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70 \n626 627 628 629 630 631 632 634 635 636 637 638 640 641 642 643 644 645 \n 70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70  70 \n\n# Get the number of assessment per day for each participant.\nbeeps.person &lt;- lapply(\n    data$PID, function(i) {\n        table(data$day[which(data$PID == i)])\n    }\n)\n\n# Show results for some of the participants.\nbeeps.person[1:6]\n\n[[1]]\n\n 1  2  3  4  5  6  7 \n10 10 10 10 10 10 10 \n\n[[2]]\n\n 1  2  3  4  5  6  7 \n10 10 10 10 10 10 10 \n\n[[3]]\n\n 1  2  3  4  5  6  7 \n10 10 10 10 10 10 10 \n\n[[4]]\n\n 1  2  3  4  5  6  7 \n10 10 10 10 10 10 10 \n\n[[5]]\n\n 1  2  3  4  5  6  7 \n10 10 10 10 10 10 10 \n\n[[6]]\n\n 1  2  3  4  5  6  7 \n10 10 10 10 10 10 10 \n\n# Compute a binary variable indicating if a participant answered a beep. We take\n# the ESM item PA as reference because in this ESM design participants were not\n# allowed to skip items.\ndata$Compliance &lt;- ifelse(is.na(data$PA) == FALSE, 1, 0)\n\n# Mean, median of the compliance across all participants.\ndescribe(data$Compliance)\n\n   vars    n mean   sd median trimmed mad min max range  skew kurtosis se\nX1    1 5460 0.88 0.32      1    0.98   0   0   1     1 -2.41     3.81  0\n\n# Compliance per participant.\ndata.compliance.person &lt;- aggregate(\n    data$Compliance,\n    by = list(data$PID),\n    mean,\n    na.rm = TRUE\n)\n\n# See the first 6 rows.\nhead(data.compliance.person)\n\n  Group.1         x\n1     101 0.9142857\n2     102 0.8857143\n3     103 0.9571429\n4     105 0.9714286\n5     108 0.6000000\n6     109 0.9857143\n\n# See the last 6 rows.\ntail(data.compliance.person)\n\n   Group.1         x\n73     640 0.7714286\n74     641 0.8571429\n75     642 0.9428571\n76     643 0.9857143\n77     644 0.7142857\n78     645 0.9571429\n\n# Obtain descriptive statistics of person's average compliance.\ndescribe(data.compliance.person$x)\n\n   vars  n mean  sd median trimmed  mad  min max range  skew kurtosis   se\nX1    1 78 0.88 0.1   0.92     0.9 0.07 0.54   1  0.46 -1.29     1.28 0.01\n\n\nNext, we obtain descriptive statistics of the distribution of the person-level or time-invariant variables.\n\n# We create a variable including the\n# diagnosis (i.e. 1 = `MDD` and 0 = control group),\n# and depression (`QIDS`) for each participant.\ndt.person &lt;- aggregate(\n    cbind(data$MDD, data$QIDS),\n    by = list(data$PID),\n    mean,\n    na.rm = TRUE\n)\n\n# Add column names.\ncolnames(dt.person) &lt;- c(\"Group.1\", \"MDD\", \"QIDS\")\n\n# See the first 6 rows.\nhead(dt.person)\n\n  Group.1 MDD QIDS\n1     101   1   12\n2     102   1   10\n3     103   1   18\n4     105   1   16\n5     108   1    5\n6     109   1   14\n\n# See the last 6 rows.\ntail(dt.person)\n\n   Group.1 MDD QIDS\n73     640   0    1\n74     641   0    4\n75     642   0    0\n76     643   0    6\n77     644   0   10\n78     645   0    4\n\n# Descriptive statistics for time-invariant variable `QIDS`.\ndescribe(dt.person$QIDS)\n\n   vars  n mean   sd median trimmed mad min max range skew kurtosis   se\nX1    1 78 9.36 7.35      8    8.92 8.9   0  24    24 0.43    -1.25 0.83\n\n# Descriptive statistics for time-invariant variable `QIDS` for `MDD = 1`.\ndescribe(dt.person$QIDS[dt.person$MDD == 1])\n\n   vars  n  mean sd median trimmed  mad min max range skew kurtosis   se\nX1    1 38 15.71  5     16   15.88 5.93   5  24    19 -0.3    -0.82 0.81\n\n# Descriptive statistics for time-invariant variable `QIDS` for `MDD = 0`.\ndescribe(dt.person$QIDS[dt.person$MDD == 0])\n\n   vars  n mean   sd median trimmed  mad min max range skew kurtosis  se\nX1    1 40 3.33 2.53      3    3.03 1.48   0  10    10 0.94     0.54 0.4\n\n\nWe now focus the time-varying variables, we obtain visualization and descriptive statistics for the time-varying variable negative affect (NA).\n\n# Histogram for the time-varying variable negative affect (i.e.  `NA.`).\nggplot(data, aes(NA.)) +\n    geom_histogram(\n        bins = 30\n    ) +\n    scale_fill_viridis() +\n    theme_bw()\n\n\n\n\n\n\n\n# Histogram for the time-varying variable `NA.` by `MDD`.\nggplot(data, aes(NA.)) +\n    geom_histogram(\n        bins = 30\n    ) +\n    facet_wrap(\n        . ~ MDD\n    ) +\n    scale_fill_viridis() +\n    theme_bw()\n\n\n\n\n\n\n\n# Descriptive statistics for `NA.`.\ndescribe(data$NA.)\n\n   vars    n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 4831 25.61 22.19   18.8   22.95 21.05   0 100   100 0.86    -0.17 0.32\n\n# Descriptive statistics for `NA.` in the `MDD` group.\ndescribe(data$NA.[data$MDD == 1])\n\n   vars    n  mean    sd median trimmed   mad min max range skew kurtosis  se\nX1    1 2255 43.28 19.22   41.2   42.57 19.57   0 100   100 0.31    -0.39 0.4\n\n# Descriptive statistics for `NA.` in the control group.\ndescribe(data$NA.[data$MDD == 0])\n\n   vars    n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 2576 10.15 9.34    7.2    8.68 6.23   0  66    66 1.75     3.77 0.18\n\n# Distribution of happy per participant.\ndata.table.dt &lt;- setDT(na.omit(data))\ndata.table.dt[, as.list(summary(NA., na.omit = TRUE)), by = PID]\n\n    PID Min. 1st Qu. Median      Mean 3rd Qu.  Max.\n 1: 101 14.4   21.40   25.9 29.621875   36.25  65.6\n 2: 102 14.4   32.40   38.2 39.090323   46.25  64.2\n 3: 103 25.4   57.50   66.6 65.528358   75.10  93.2\n 4: 105 25.2   42.80   50.7 50.235294   57.40  77.8\n 5: 108  9.0   13.40   17.1 17.647619   21.35  37.6\n 6: 109 11.6   25.80   32.2 33.956522   43.20  60.8\n 7: 110  7.8   20.60   29.4 31.664615   40.60  68.6\n 8: 111 13.2   24.40   26.3 26.767647   30.05  45.6\n 9: 112 47.4   62.50   66.2 68.817910   73.90  92.2\n10: 114  5.6   27.85   35.3 36.342424   44.45  73.8\n11: 115 18.0   42.40   56.2 57.168627   77.80  84.2\n12: 117 22.2   27.80   35.4 34.553125   40.05  51.4\n13: 119 12.8   26.80   38.6 37.927869   46.00  91.0\n14: 120 24.4   35.30   40.4 40.857576   46.60  54.6\n15: 121 31.6   44.85   49.2 50.421429   56.65  73.0\n16: 124 15.0   27.60   33.4 34.266667   39.80  69.0\n17: 125 13.8   19.00   23.2 23.567347   27.20  39.4\n18: 128  8.0   33.70   49.1 45.072414   58.85  69.8\n19: 130 10.8   32.40   52.8 49.652830   68.40  83.6\n20: 131 19.0   38.80   53.6 49.932308   60.00  75.0\n21: 132 15.2   32.60   43.4 43.393333   52.10  81.4\n22: 135 18.0   47.55   57.4 55.561765   64.60  91.2\n23: 136  0.0    4.85   15.5 25.842424   35.85 100.0\n24: 138 25.0   40.55   47.8 47.955882   53.40  76.6\n25: 139 22.0   35.20   42.0 41.487719   48.20  57.0\n26: 140 38.8   47.50   52.7 52.700000   57.35  78.0\n27: 141 18.4   30.35   37.3 36.042105   40.50  50.8\n28: 143 59.4   72.60   75.1 74.590909   77.55  82.4\n29: 144 45.2   50.75   53.1 54.143333   55.25  74.2\n30: 202  5.0   27.30   34.4 35.142424   40.25  66.6\n31: 204 22.6   28.35   34.3 35.153846   40.95  65.2\n32: 205 15.8   28.35   34.4 34.539286   42.25  52.6\n33: 206  3.6    7.05    9.4 13.531034   18.70  50.6\n34: 207 27.2   78.40   82.4 81.207547   88.60 100.0\n35: 208 25.0   56.00   64.6 63.627692   71.80  96.4\n36: 210 15.4   32.75   40.5 44.350000   53.40  84.6\n37: 211 27.2   34.40   38.8 39.609836   43.40  56.6\n38: 306  5.2   21.00   30.2 30.680702   39.80  67.2\n39: 601  0.0    2.40    5.1  9.190909   10.95  42.6\n40: 602  2.0    4.40    6.0  6.453731    7.80  15.4\n41: 603  0.0    6.15   14.1 16.556250   24.00  56.6\n42: 604  3.2    9.80   12.8 13.620000   16.15  39.2\n43: 606  0.0    6.80    9.1  9.225000   12.00  18.6\n44: 607  0.0    0.40   10.2 12.200000   20.30  63.0\n45: 608  0.0    1.00    8.0 10.140299   14.20  52.6\n46: 609  4.8   24.00   29.2 29.714286   37.00  48.2\n47: 610  5.6   16.95   24.6 26.059375   33.05  66.0\n48: 611  1.6    4.00    5.6  6.552941    7.10  26.0\n49: 612  2.2    3.40    5.2  5.857143    6.90  17.6\n50: 613  0.0    0.00    1.6  3.609231    3.80  36.2\n51: 614  3.2    8.80   10.4 10.865625   13.20  18.8\n52: 615  0.0    0.00    0.0  1.368571    0.20  37.0\n53: 616  0.0    0.00    2.7  5.758824    9.00  40.4\n54: 618  0.0    2.20    3.4  4.174194    4.60  17.4\n55: 619  1.0    3.50    5.0  5.114286    6.55  14.6\n56: 621  9.8   16.10   20.2 20.682353   24.10  32.8\n57: 622  7.6   13.55   16.2 16.515625   19.35  29.0\n58: 623  2.0    5.40    6.6  7.442857    8.60  23.4\n59: 624  2.4    6.40    9.5 10.533333   14.75  21.4\n60: 625  0.8    5.40    7.8 14.073846   17.40  47.0\n61: 626  0.8    2.20    3.6  6.636066    5.60  48.8\n62: 627  0.0    1.65    4.0  5.719355    8.10  27.0\n63: 628  0.8    3.40    5.2  6.206154    6.60  25.2\n64: 629  2.6    8.80   12.6 13.993846   17.20  41.0\n65: 630  1.2    4.60    6.0 10.284848   10.90  37.6\n66: 631  1.6    3.80    6.0  6.057143    7.40  16.6\n67: 632  3.2    7.55   13.0 12.473333   14.45  40.0\n68: 634  0.0    1.30    3.6  4.208955    5.80  22.0\n69: 635  1.8    4.35    5.3  7.062500    6.20  38.0\n70: 636  0.0    4.05    6.0  6.120000    8.15  12.4\n71: 637  2.2    4.60    5.8  6.520000    7.20  22.4\n72: 638  0.0    6.05    9.1  9.533333   11.85  22.4\n73: 640  2.8    5.40    7.2  8.114815    9.75  17.6\n74: 641  0.0    2.75    8.1  9.723333   14.15  33.8\n75: 642  0.0    0.00    0.5  5.451515    8.55  34.6\n76: 643  4.2    8.40   11.0 12.075362   14.20  29.4\n77: 644 11.0   17.20   19.7 20.696000   21.95  44.4\n78: 645  5.0    9.60   12.2 17.811940   19.10  54.4\n    PID Min. 1st Qu. Median      Mean 3rd Qu.  Max.\n\n# We randomly select 10 participants for plotting the\n# distribution of the time-varying variable `NA.`.\nn.ID.sample &lt;- sample(unique(data$PID), 4)\ndata.person.sample &lt;- data[which(data$PID %in% n.ID.sample), ]\n\n# Histogram for the time-varying variable happy by person\nggplot(data.person.sample, aes(NA.)) +\n    geom_histogram(color = \"black\", fill = \"white\", bins = 30) +\n    facet_wrap(~PID) +\n    scale_fill_viridis() +\n    theme_bw()\n\n\n\n\n\n\n\n# Plot the trajectories of the time-varying variable NA by person\ndata.person.sample %&gt;%\n    ggplot(aes(x = obs, y = NA.)) +\n    geom_point() +\n    geom_line() + # add lines to connect the data for each person\n    facet_wrap(. ~ PID) +\n    scale_fill_viridis() +\n    theme_bw()\n\n\n\n\n\n\n\n# We create a variable including the `MDD` (1 = `MDD`, 0 = control group),\n# and person's means of the time-varying variable `NA.`.\ndt.person &lt;- aggregate(\n    cbind(data$MDD, data$NA.),\n    by = list(data$PID),\n    mean,\n    na.rm = TRUE\n)\n\n# Add column names.\ncolnames(dt.person) &lt;- c(\"Group.1\", \"MDD\", \"NA.\")\n\n# See the first 6 rows.\nhead(dt.person)\n\n  Group.1 MDD      NA.\n1     101   1 29.62187\n2     102   1 39.09032\n3     103   1 65.52836\n4     105   1 50.23529\n5     108   1 17.64762\n6     109   1 33.95652\n\n# See the last 6 rows.\ntail(dt.person)\n\n   Group.1 MDD       NA.\n73     640   0  8.114815\n74     641   0  9.723333\n75     642   0  5.451515\n76     643   0 12.075362\n77     644   0 20.696000\n78     645   0 17.811940\n\n# Descriptive statistics for person's means of the time-varying variable `NA.`.\ndescribe(dt.person$NA.)\n\n   vars  n  mean    sd median trimmed   mad  min   max range skew kurtosis   se\nX1    1 78 26.24 19.91  20.69   24.23 21.06 1.37 81.21 79.84 0.72    -0.45 2.25\n\n# Descriptive statistics for person's means of the time-varying variable `NA.`\n# for `MDD` = 1.\ndescribe(dt.person$NA.[dt.person$MDD == 1])\n\n   vars  n  mean    sd median trimmed   mad   min   max range skew kurtosis\nX1    1 38 42.96 15.02  40.23   42.29 14.06 13.53 81.21 67.68 0.51    -0.06\n     se\nX1 2.44\n\n# Descriptive statistics for person's means of the time-varying variable `NA.`\n# for `MDD` = 0.\ndescribe(dt.person$NA.[dt.person$MDD == 0])\n\n   vars  n  mean   sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 40 10.36 6.14   9.21     9.5 4.76 1.37 29.71 28.35 1.27     1.32 0.97\n\n# We create a variable including the `MDD` (1 = `MDD`, 0 = control group),\n# and person's standard deviation of the time-varying variable `NA.`.\ndt.person.sd &lt;- aggregate(\n    data$NA.,\n    by = list(data$PID, data$MDD),\n    sd,\n    na.rm = TRUE\n)\n\n# Add column names.\ncolnames(dt.person.sd) &lt;- c(\"Group.1\", \"MDD\", \"NA.\")\n\n# See the first 6 rows.\nhead(dt.person.sd)\n\n  Group.1 MDD       NA.\n1     601   0 10.139970\n2     602   0  2.905915\n3     603   0 12.948615\n4     604   0  6.165745\n5     606   0  3.925499\n6     607   0 12.595571\n\n# See the last 6 rows.\ntail(dt.person.sd)\n\n   Group.1 MDD       NA.\n73     206   1 10.115789\n74     207   1 12.116928\n75     208   1 12.489439\n76     210   1 17.420019\n77     211   1  6.747881\n78     306   1 14.098726\n\n# Descriptive statistics for person's standard deviation of the time-varying\n# variable `NA.`.\ndescribe(dt.person.sd$NA.)\n\n   vars  n mean   sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 78 8.97 4.67   8.67    8.56 5.22 2.14 27.32 25.18 1.06     1.81 0.53\n\n# Descriptive statistics for person's standard deviation of the time-varying\n# variable `NA.` for `MDD` = 1.\ndescribe(dt.person.sd$NA.[dt.person.sd$MDD == 1])\n\n   vars  n  mean  sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 38 11.44 4.7  10.98      11 3.59 4.16 27.32 23.16  1.1     1.76 0.76\n\n# Descriptive statistics for person's standard deviation of the time-varying\n# variable `NA.` for `MDD` = 0.\ndescribe(dt.person.sd$NA.[dt.person.sd$MDD == 0])\n\n   vars  n mean   sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 40 6.63 3.24   5.62    6.35 3.29 2.14 12.95  10.8 0.62    -0.89 0.51"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html#example-1",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html#example-1",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "Example 1",
    "text": "Example 1\nEstimating the effect of a continuous time-varying predictor\nThe first illustrative example shows how to estimate the effect of a time-varying predictor on the outcome of interest. Considering the Leuven clinical study, we are interested in studying the impact of anhedonia on negative affect in daily life on patients with major depressive disorder.\nWe use the data of \\(38\\) individuals diagnosed with MDD. We select the individuals diagnosed with MDD.\n\n# Create `MDD` subset.\ndata.MDD &lt;- data[which(data$MDD == 1), ]\n\nFirst, we are going to estimate the individual means, the mean across all participants, and the standard deviation of the variable anhedonia.\n\n# Compute the group mean of anhedonia.\ngroupmean_X = aggregate(\n    data.MDD$anhedonia,\n    list(data.MDD$PID),\n    FUN = mean,\n    data = data.MDD,\n    na.rm = TRUE\n)\n\n# Compute the mean.\nmean_X &lt;- mean(groupmean_X[, 2])\n\n# Print the mean.\nprint(mean_X)\n\n[1] 51.66162\n\n# Compute the standard deviation.\nsd_X &lt;- sd(data.MDD$anhedonia, na.rm = TRUE)\n\n# Print the standard deviation.\nprint(sd_X)\n\n[1] 23.6734\n\n\nNext, we are going to person mean-centered the variable anhedonia.\n\n# Centered within individuals anhedonia.\nN.i &lt;- unique(data.MDD$PID)\nanhedonia.c = rep(0, nrow(data.MDD))\n\nfor (i in N.i) {\n    # Get the anhedonia for the i-th individual.\n    ith_anhedonia &lt;- data.MDD$anhedonia[which(data.MDD$PID == i)]\n\n    # Get the mean of anhedonia for the i-th individual.\n    ith_anhedonia_mean &lt;- mean(data.MDD$anhedonia[which(data.MDD$PID == i)], na.rm = TRUE)\n\n    # Center.\n    anhedonia.c[which(data.MDD$PID == i)] &lt;- ith_anhedonia - ith_anhedonia_mean\n}\n\n# Add the centered variable to the data.\ndata.MDD &lt;- cbind(data.MDD, anhedonia.c)\n\nWe estimate the linear mixed-effects model assuming \\(\\text{AR}(1)\\) errors:\n\n# Fit a linear mixed-effects model to data.\nfit.Model.1 = lme(\n    fixed = NA. ~ 1 + anhedonia.c,\n    random = ~ 1 + anhedonia.c | PID,\n    na.action = na.omit,\n    data = data.MDD,\n    correlation = corAR1(),\n    method = \"REML\"\n)\n\nThe summary of the estimation results is given by:\n\n# Summary of the estimation results.\nsummary(fit.Model.1)\n\nLinear mixed-effects model fit by REML\n  Data: data.MDD \n       AIC      BIC    logLik\n  17319.52 17359.56 -8652.758\n\nRandom effects:\n Formula: ~1 + anhedonia.c | PID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 14.7788373 (Intr)\nanhedonia.c  0.1162717 0.003 \nResidual    11.9150994       \n\nCorrelation Structure: AR(1)\n Formula: ~1 | PID \n Parameter estimate(s):\n      Phi \n0.4293834 \nFixed effects:  NA. ~ 1 + anhedonia.c \n               Value Std.Error   DF   t-value p-value\n(Intercept) 42.98279 2.4299657 2216 17.688641       0\nanhedonia.c  0.13900 0.0233386 2216  5.955752       0\n Correlation: \n            (Intr)\nanhedonia.c 0.002 \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.21865272 -0.56951096 -0.04394045  0.53168368  6.16917066 \n\nNumber of Observations: 2255\nNumber of Groups: 38 \n\n\nObtain confidence intervals:\n\n# Confidence intervals.\nintervals(fit.Model.1, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                  lower       est.      upper\n(Intercept) 38.21754213 42.9827900 47.7480379\nanhedonia.c  0.09323107  0.1389989  0.1847667\n\n\nThe estimated fixed intercept is given by:\n\n# Extract fixed effect coefficients.\n# Extract the value of fixed intercept.\ncoef(summary(fit.Model.1))[1, 1]\n\n[1] 42.98279\n\n\nthe effect of the level 2 continuous variable on the intercept is extracted as follows:\n\n# Extract the value of the fixed slope.\ncoef(summary(fit.Model.1))[2, 1]\n\n[1] 0.1389989\n\n\nThe standard deviation and autocorrelation of the level 1 residuals are extracted as follows:\n\n# Extract level 1 residuals standard deviation.\nas.numeric(VarCorr(fit.Model.1)[3, 2])\n\n[1] 11.9151\n\n# Extract level 1 residuals correlation between consecutive points\nas.numeric(coef(\n    fit.Model.1$modelStruct$corStruct,\n    unconstrained = FALSE\n))\n\n[1] 0.4293834\n\n\nThe standard deviation of the random intercept is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random intercept.\nas.numeric(VarCorr(fit.Model.1)[1, 2])\n\n[1] 14.77884\n\n\nThe standard deviation of the random slope is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random slope.\nas.numeric(VarCorr(fit.Model.1)[2, 2])\n\n[1] 0.1162717\n\n\nThe correlation between the random intercept and the random slope is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random slope.\nas.numeric(VarCorr(fit.Model.1)[2, 3])\n\n[1] 0.003"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html#example-2",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html#example-2",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "Example 2",
    "text": "Example 2\nEstimating cross-level interaction effect between a continuous time-varying predictor and a continuous time-invariant predictor\nWe now show how to estimate a cross-level interaction effect between a continuous time-varying predictor and continuous time-invariant predictor. In particular, we are interested in studying if depression moderated the impact of anhedonia on negative affect in daily life for individuals diagnosed with MDD.\nBefore estimating the model, we are going to compute the mean and standard deviation of the level 2 variable QIDS.\n\n# Compute the mean of `W`.\ngroupmean_W = aggregate(\n    data.MDD$QIDS,\n    list(data.MDD$PID),\n    FUN = mean,\n    data = data.Controls,\n    na.rm = TRUE,\n    method = \"REML\"\n)\n\n# Compute the mean.\nmean_W &lt;- mean(groupmean_W[, 2])\n\n# Print the mean.\nprint(mean_W)\n\n[1] 15.71053\n\n# Compute the standard deviation.\nsd_W &lt;- sd(groupmean_W[, 2])\n\n# Print the standard deviation.\nprint(sd_W)\n\n[1] 4.996798\n\n\nNext, we are going to mean centered the variable QIDS using the mean estimated above:\n\n# Centered QIDS.\nN.i &lt;- unique(data.MDD$PID)\nQIDS.c &lt;- rep(0, nrow(data.MDD))\n\n# For each participant.\nfor (i in N.i) {\n    # Extract the value of the variable for the i-th individual.\n    ith_QIDS &lt;- data.MDD$QIDS[which(data.MDD$PID == i)]\n\n    # Center the variable.\n    QIDS.c[which(data.MDD$PID == i)] &lt;- ith_QIDS - mean_W\n}\n\n# Add the centered variable to the data.\ndata.MDD &lt;- cbind(data.MDD, QIDS.c)\n\nNext, we estimate the linear mixed-effects model assuming \\(\\text{AR}(1)\\) errors:\n\n# Fit a linear mixed-effects model to data.\nfit.Model.2 = lme(\n    fixed = NA. ~ 1 + anhedonia.c + anhedonia.c * QIDS.c,\n    random = ~ 1 + anhedonia.c | PID,\n    na.action = na.omit,\n    data = data.MDD,\n    correlation = corAR1(),\n    method = \"REML\"\n)\n\nThe summary of the estimation results is given by:\n\n# Print the summary of the estimation results.\nsummary(fit.Model.2)\n\nLinear mixed-effects model fit by REML\n  Data: data.MDD \n       AIC      BIC    logLik\n  17315.42 17366.89 -8648.708\n\nRandom effects:\n Formula: ~1 + anhedonia.c | PID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev    Corr  \n(Intercept) 12.855527 (Intr)\nanhedonia.c  0.105615 0.249 \nResidual    11.923407       \n\nCorrelation Structure: AR(1)\n Formula: ~1 | PID \n Parameter estimate(s):\n     Phi \n0.430249 \nFixed effects:  NA. ~ 1 + anhedonia.c + anhedonia.c * QIDS.c \n                      Value Std.Error   DF   t-value p-value\n(Intercept)        42.97796 2.1228674 2215 20.245238  0.0000\nanhedonia.c         0.13747 0.0218390 2215  6.294570  0.0000\nQIDS.c              1.52600 0.4308467   36  3.541864  0.0011\nanhedonia.c:QIDS.c -0.01019 0.0046382 2215 -2.197917  0.0281\n Correlation: \n                   (Intr) anhdn. QIDS.c\nanhedonia.c         0.191              \nQIDS.c             -0.001  0.000       \nanhedonia.c:QIDS.c  0.000 -0.031  0.183\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.20450995 -0.56155161 -0.03764376  0.53518590  6.15760197 \n\nNumber of Observations: 2255\nNumber of Groups: 38 \n\n\nObtain confidence intervals:\n\n# Print the confidence intervals.\nintervals(fit.Model.2, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                         lower        est.        upper\n(Intercept)        38.81493867 42.97795723 47.140975796\nanhedonia.c         0.09464004  0.13746708  0.180294127\nQIDS.c              0.65220263  1.52600019  2.399797755\nanhedonia.c:QIDS.c -0.01929006 -0.01019438 -0.001098702\n\n\nThe estimated fixed intercept is given by:\n\n# Extract fixed effect coefficients.\n# Extract the value of fixed intercept.\ncoef(summary(fit.Model.2))[1, 1]\n\n[1] 42.97796\n\n\nThe effect of the level 2 continuous variable on the intercept is extracted as follows:\n\n# Extract the value of the fixed slope.\ncoef(summary(fit.Model.2))[2, 1]\n\n[1] 0.1374671\n\n\nThe effect of the level 2 continuous variable on the intercept is extracted as follows:\n\n# Extract the value of the fixed slope.\ncoef(summary(fit.Model.2))[3, 1]\n\n[1] 1.526\n\n\nThe effect of the level 2 continuous variable on the intercept is extracted as follows:\n\n# Extract the value of the fixed slope.\ncoef(summary(fit.Model.2))[4, 1]\n\n[1] -0.01019438\n\n\nThe standard deviation and autocorrelation of the level 1 residuals are extracted as follows:\n\n# Extract level 1 residuals standard deviation.\nas.numeric(VarCorr(fit.Model.2)[3, 2])\n\n[1] 11.92341\n\n# Extract level 1 residuals correlation between consecutive points.\nas.numeric(coef(\n    fit.Model.2$modelStruct$corStruct,\n    unconstrained = FALSE\n))\n\n[1] 0.430249\n\n\nThe standard deviation of the random intercept is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random intercept.\nas.numeric(VarCorr(fit.Model.2)[1, 2])\n\n[1] 12.85553\n\n\nThe standard deviation of the random slope is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random slope.\nas.numeric(VarCorr(fit.Model.2)[2, 2])\n\n[1] 0.105615\n\n\nThe correlation between the random intercept and the random slope is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random slope.\nas.numeric(VarCorr(fit.Model.2)[2, 3])\n\n[1] 0.249"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html#example-3",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html#example-3",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "Example 3",
    "text": "Example 3\nEstimate group differences in the autoregressive effect in multilevel \\(\\text{AR}(1)\\) models\nIn this illustration, we are interested in estimating differences in the autoregressive effect of negative affect between participants diagnosed with major depressive disorder (MDD) and control subjects. The dataset contains 38 participants diagnosed with MDD and 40 control subjects.\nFirst, for each individual, we are going to compute the lagged variable negative affect (i.e., NA.). The variable negative affect is lagged within each day.\n\n# Create a lag variable.\n# The data is lag within a person and days.\nNA.lag &lt;- rep(0, nrow(data))\nsubjno.i &lt;- unique(data$PID)\n\n# For each subject.\nfor (i in subjno.i) {\n    n.i = which(data$PID == i)\n    Day.i = data$day[n.i]\n\n    # For each day.\n    for (t in unique(Day.i)) {\n        k.i = n.i[which(data$day[n.i] == t)]\n        NA.lag[k.i] = shift(data$NA.[k.i], 1)\n    }\n}\n\n# Add the lagged variable to the data.\ndata &lt;- cbind(data, NA.lag)\n\nThe lagged variable NA.lag will be centered using the individual‚Äôs mean.\n\n# Centered within individuals NA.lag.\nN.i &lt;- unique(data$PID)\nNA.lag.c &lt;- rep(0, nrow(data))\n\n# For each individual.\nfor (i in N.i) {\n    # Get the `NA.lag` for the i-th individual.\n    ith_na_lag &lt;- data$NA.lag[which(data$PID == i)]\n\n    # Get the `NA.lag` mean for the i-th individual.\n    ith_na_lag_mean &lt;- mean(data$NA.[which(data$PID == i)], na.rm = TRUE)\n\n    # Center.\n    NA.lag.c[which(data$PID == i)] &lt;- ith_na_lag - ith_na_lag_mean\n}\n\n# Add the centered lagged variable to the data.\ndata &lt;- cbind(data, NA.lag.c)\n\nTo estimate the model, we use the function lme from the nlme R package. The dependent variable is the negative affect (i.e.¬†NA.), the predictor is the lagged outcome, which is centered using the individuals‚Äô mean:\n\n# Fit a linear mixed-effects model to data.\nfit.Model.3 &lt;- lme(\n    fixed = NA. ~ 1 + MDD + NA.lag + MDD * NA.lag,\n    random = ~ 1 + NA.lag | PID,\n    na.action = na.omit,\n    data = data,\n    method = \"REML\"\n)\n\nwhere NA. is the negative affect, 1 is the fixed intercept, MDD is the difference in the fixed intercept between the two groups, NA.lag.c is the fixed autoregressive effect and MDD*NA.lag.c is the difference in the fixed autoregressive effect between the two groups. The random effect structure of the model is 1 + NA.lag.c|PID, where 1 is the random intercept, and NA.lag.c is the random slope, which is allowed to vary over participants (PID).\nThe summary of the estimation results is given by:\n\n# Print the summary of the model.\nsummary(fit.Model.3)\n\nLinear mixed-effects model fit by REML\n  Data: data \n       AIC      BIC    logLik\n  28968.54 29018.86 -14476.27\n\nRandom effects:\n Formula: ~1 + NA.lag | PID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev    Corr  \n(Intercept) 5.7874515 (Intr)\nNA.lag      0.1402728 -0.199\nResidual    8.7540299       \n\nFixed effects:  NA. ~ 1 + MDD + NA.lag + MDD * NA.lag \n                Value Std.Error   DF   t-value p-value\n(Intercept)  6.824841 0.9800415 3911  6.963828  0.0000\nMDD         16.326601 1.5896208   76 10.270752  0.0000\nNA.lag       0.313887 0.0366665 3911  8.560571  0.0000\nMDD:NA.lag   0.116184 0.0472240 3911  2.460275  0.0139\n Correlation: \n           (Intr) MDD    NA.lag\nMDD        -0.617              \nNA.lag     -0.339  0.209       \nMDD:NA.lag  0.263 -0.417 -0.776\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-5.5027244 -0.4822533 -0.1062090  0.3948620  6.3499125 \n\nNumber of Observations: 3991\nNumber of Groups: 78 \n\n\nWe extract the estimated fixed intercept as follows,\n\n# Extract fixed effect coefficients.\n# Extract the value of fixed intercept.\ncoef(summary(fit.Model.3))[1, 1]\n\n[1] 6.824841\n\n\nThe differences on the intercept between the two groups is given by:\n\n# Extract the value of the difference in the fixed intercept between the two\n# groups.\ncoef(summary(fit.Model.3))[2, 1]\n\n[1] 16.3266\n\n\nThe fixed autoregressive effect is:\n\n# Extract the value of fixed slope.\ncoef(summary(fit.Model.3))[3, 1]\n\n[1] 0.3138866\n\n\nAnd the difference in the autoregressive effect between the two groups is extracted as follows:\n\n# Extract the value of the difference in the fixed slope between\n# the two groups.\ncoef(summary(fit.Model.3))[4, 1]\n\n[1] 0.1161839\n\n\nThe standard deviation of the level 1 residuals is extracted as follows:\n\n# Extract level 1 residuals standard deviation.\nas.numeric(VarCorr(fit.Model.3)[3, 2])\n\n[1] 8.75403\n\n\nThe standard deviation of the random intercept is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random intercept.\nas.numeric(VarCorr(fit.Model.3)[1, 2])\n\n[1] 5.787452\n\n\nThe standard deviation of the random slope is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random slope.\nas.numeric(VarCorr(fit.Model.3)[2, 2])\n\n[1] 0.1402728\n\n\nThe correlation between the random intercept and the random slope is given by:\n\n# Extract random effect covariance structure.\n# Extract the standard deviation of the random slope.\nas.numeric(VarCorr(fit.Model.3)[2, 3])\n\n[1] -0.199"
  },
  {
    "objectID": "exercises/estimation-multilevel-leuven-clinical-data.html#session-information",
    "href": "exercises/estimation-multilevel-leuven-clinical-data.html#session-information",
    "title": "Multilevel Model Estimation Using the Lueven Clinical Data Set",
    "section": "Session information",
    "text": "Session information\nUsing the command below, we can print the session information (i.e., operating system, details about the R installation, and so on) for reproducibility purposes.\n\n\n# Session information.\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] nlme_3.1-162      viridis_0.6.3     viridisLite_0.4.2 psych_2.3.3      \n [5] data.table_1.14.8 lubridate_1.9.2   forcats_1.0.0     stringr_1.5.0    \n [9] dplyr_1.1.2       purrr_1.0.1       readr_2.1.4       tidyr_1.3.0      \n[13] tibble_3.2.1      ggplot2_3.4.2     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.3        generics_0.1.3    stringi_1.7.12    lattice_0.21-8   \n [5] hms_1.1.3         digest_0.6.31     magrittr_2.0.3    evaluate_0.21    \n [9] grid_4.3.0        timechange_0.2.0  fastmap_1.1.1     jsonlite_1.8.5   \n[13] gridExtra_2.3     fansi_1.0.4       scales_1.2.1      mnormt_2.1.1     \n[17] cli_3.6.1         rlang_1.1.1       munsell_0.5.0     withr_2.5.0      \n[21] yaml_2.3.7        tools_4.3.0       parallel_4.3.0    tzdb_0.4.0       \n[25] colorspace_2.1-0  vctrs_0.6.2       R6_2.5.1          lifecycle_1.0.3  \n[29] htmlwidgets_1.6.2 pkgconfig_2.0.3   pillar_1.9.0      gtable_0.3.3     \n[33] glue_1.6.2        xfun_0.39         tidyselect_1.2.0  rstudioapi_0.14  \n[37] knitr_1.43        farver_2.1.1      htmltools_0.5.5   labeling_0.4.2   \n[41] rmarkdown_2.22    compiler_4.3.0"
  },
  {
    "objectID": "exercises/power-multilevel-ar-intensive-designs.html",
    "href": "exercises/power-multilevel-ar-intensive-designs.html",
    "title": "Power Analysis for \\(\\text{AR}(1)\\) Multilevel Models in Intensive Longitudinal Designs",
    "section": "",
    "text": "This file needs attention!"
  },
  {
    "objectID": "exercises/power-multilevel-ar-intensive-designs.html#settings-things-up",
    "href": "exercises/power-multilevel-ar-intensive-designs.html#settings-things-up",
    "title": "Power Analysis for \\(\\text{AR}(1)\\) Multilevel Models in Intensive Longitudinal Designs",
    "section": "Settings things up",
    "text": "Settings things up\nBefore we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\nhtmltools\nDT\nnlme\nggplot2\ngridExtra\ndata.table\nplyr\ndplyr\nformattable\ntidyr\nMASS\nkableExtra\nfuture.apply\nPowerAnalysisIL\ntictoc\nviridis\nggpubr\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\n    \"htmltools\", \"DT\", \"nlme\", \"ggplot2\", \"gridExtra\",\n    \"data.table\", \"plyr\", \"dplyr\", \"formattable\", \"tidyr\",\n    \"MASS\", \"kableExtra\", \"future.apply\", \"tictoc\",\n    \"viridis\", \"ggpubr\", \"devtools\"\n)\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nAt last, we can load the packages into our R session:\n\nlibrary(htmltools)\nlibrary(DT)\nlibrary(nlme)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(data.table)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(formattable)\nlibrary(tidyr)\nlibrary(MASS)\nlibrary(future.apply)\nlibrary(tictoc)\nlibrary(kableExtra)\nlibrary(viridis)\nlibrary(ggpubr)\n\nFinally, we will also install the Shiny application package PowerAnalysisIL because it contains the functions we will use to conduct the power analysis.\n\n# Install the shiny application package.\ndevtools::install_github(\"ginettelafit/PowerAnalysisIL\")\n\n## Load package `PowerAnalysisIL`.\nlibrary(PowerAnalysisIL)"
  },
  {
    "objectID": "exercises/power-multilevel-ar-intensive-designs.html#description",
    "href": "exercises/power-multilevel-ar-intensive-designs.html#description",
    "title": "Power Analysis for \\(\\text{AR}(1)\\) Multilevel Models in Intensive Longitudinal Designs",
    "section": "Description",
    "text": "Description\nWe now focus on conducting a sensitivity power analysis to investigate different factors that influence statistical power when testing group differences in the autoregressive effect.\nThe goal of the new study is to determine the sample size (i.e., \\(N_0\\) the number of control participants and \\(N_1\\) the number of participants diagnosed with depression), when assuming the new study will include 70 repeated measurement occasions.\nTo estimate this research question we will use a multilevel \\(\\text{AR}(1)\\) model with a cross-level interaction effect between a level 1 continuous predictor (lagged NA) and level 2 binary predictor (diagnosis):\n\\[\\begin{multline}\n    \\text{NA}_{it} = \\beta_{00} + \\beta_{01} \\text{Diagnosis}_{i} + \\beta_{10} \\text{NA}_{it-1} + \\\\\n    \\beta_{01} \\text{Diagnosis}_{i}\\text{NA}_{it-1} + \\nu_{0i} + \\nu_{1i} \\text{NA}_{it-1} + \\epsilon_{it}\n\\end{multline}\\]\nwhere \\(\\beta_{00}\\) is the fixed intercept, \\(\\beta_{01}\\) represents the main effect of Diagnosis, \\(\\beta_{10}\\) is the fixed autoregressive effect, and \\(\\beta_{11}\\) represents the differences in the autoregressive effect between persons diagnosed with depression and control participants. The level 1 predictor (lagged NA) is centered within-persons and within-days.\nIn the model above, we assume the level 1 errors are independent and normally distributed with mean zero and variance \\(\\sigma_\\epsilon^2\\).\nBetween-person differences in the autoregressive effect are captured by including a random intercept \\(\\nu_{0i}\\) and random slope \\(\\nu_{1i}\\). These random effects are multivariate normal distributed with mean zero and covariance matrix \\(\\Sigma_\\nu\\):\n\\[\n\\boldsymbol{\\Sigma}_\\nu = \\begin{bmatrix}\n\\sigma_{\\nu_{0}}^2 & \\sigma_{\\nu_{01}}  \\\\\n\\sigma_{\\nu_{01}} & \\sigma_{\\nu_{1}}^2\n\\end{bmatrix}\n\\]\nIn this model, it is also assumed that the level 2 random effects and the level 1 errors are independent.\nTo investigate group differences in the autoregressive effect of NA between persons diagnosed with depression and control participants we are going to conduct the following hypothesis test:\n\\[\nH_0: \\beta_{11} = 0\n\\]\n\\[\nH_1: \\beta_{11} \\neq 0\n\\]\nThe goal of this exercise is to conduct a sensitivity power analysis to investigate:\n\ndifferences in statistical power when varying the number of measurement occasions \\(T\\) due to different levels of compliance (i.e., 60% and 80%)\ndifferences in statistical power when varying the value of \\(\\beta_{11}\\): we assume \\(\\beta_11\\) is 10% lower/higher than the one obtained using the Leuven clinical data set"
  },
  {
    "objectID": "exercises/power-multilevel-ar-intensive-designs.html#determining-model-parameter-values",
    "href": "exercises/power-multilevel-ar-intensive-designs.html#determining-model-parameter-values",
    "title": "Power Analysis for \\(\\text{AR}(1)\\) Multilevel Models in Intensive Longitudinal Designs",
    "section": "Determining model parameter values",
    "text": "Determining model parameter values\nTo obtain the values of the model parameters we will use data from the Leuven clinical study. The code to estimate the multilevel model is included in the exercise Multilevel model estimation using the Leuven Clinical Dataset. The output of the fitted model is:\n\n\n\nEstimated model parameter values\n\n\nThus, the values of the model parameter that will be used to conduct the power analysis are:\n\\[\n\\begin{aligned}\n    \\beta_{00} &= 6.82 \\; \\; \\; \\text{fixed intercept} \\\\\n    \\beta_{01} &= 16.33 \\; \\; \\; \\text{main effect of diagnosis} \\\\\n    \\beta_{10} &= 0.31 \\; \\; \\; \\text{fixed autoregressive effect} \\\\\n    \\beta_{10} &= 0.12 \\; \\; \\; \\text{cross-level interaction effect} \\\\\n    \\sigma_\\epsilon &= 8.75 \\; \\; \\; \\text{std. deviation level 1 errors} \\\\\n    \\sigma_{\\nu_0} &= 5.79 \\; \\; \\; \\text{std. deviation random intercept} \\\\\n    \\sigma_{\\nu_1} &= 0.14 \\; \\; \\; \\text{std. deviation random slope} \\\\\n    \\rho_{\\nu_{01}} &= -0.199 \\; \\; \\; \\text{correlation between the random effects}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "exercises/power-multilevel-ar-intensive-designs.html#set-the-model-parameters-values",
    "href": "exercises/power-multilevel-ar-intensive-designs.html#set-the-model-parameters-values",
    "title": "Power Analysis for \\(\\text{AR}(1)\\) Multilevel Models in Intensive Longitudinal Designs",
    "section": "Set the model parameters values",
    "text": "Set the model parameters values\nFor our sensitivity power analysis, we can use the estimated parameter values using the Leuven clinical data set:\n\n# Number of persons in reference group (Control participants).\nN.0 = c(20, 40, 60, 100)\n\n# Number of participants diagnosed with MDD.\nN.1 = c(20, 40, 60, 100)\n\n# Number of measurement occasions.\nT.obs = 70\n\n# If Ylag.center = TRUE, person-mean centered the lagged outcome, otherwise the lagged predictor is not person-mean centered.\nYlag.center = FALSE\n\n# Fixed intercept for participants in the reference group.\nb00 = 6.82\n\n# Differences in the fixed intercept between the two groups.\nb01.Z = 16.33\n\n# Fixed autoregressive effect for participants in the reference group.\nb10 = 0.31\n\n# Differences in the fixed autoregressive effect between the two groups.\nb11.Z = 0.12\n\n# Std. deviation of the level 1 errors.\nsigma = 8.75\n\n# Std. deviation of the random intercept.\nsigma.v0 = 5.79\n\n# Std. deviation of the random autoregressive effect.\nsigma.v1 = 0.14\n\n# Correlation between the random intercept and random autoregressive effect.\nrho.v = -0.199\n\n# Significant level.\nalpha = 0.05\n\n# Select the side of the test as follows.\n\n# One-side test H1: b01.Z &gt; 0\n# side.test = 1\n\n# One-side test H1: b01.Z &lt; 0\n# side.test = 2\n\n# One-side test H1: b01.Z is different from 0\nside.test = 3\n\n# Set the optimization method for `lme4`.\nOpt.Method = \"REML\"\n\n# Number of Monte Carlo replicates.\nR = 10\n\n# Set seed or the random number generator for reproducibility.\nset.seed(123)\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the code above we set the number of replications R to \\(10\\). This is just so we can compile the document quickly. In practice, you should set R to a much higher number. We recommend at least \\(1000\\) replications."
  },
  {
    "objectID": "exercises/power-multilevel-ar-intensive-designs.html#conduct-the-sensitivity-power-analysis",
    "href": "exercises/power-multilevel-ar-intensive-designs.html#conduct-the-sensitivity-power-analysis",
    "title": "Power Analysis for \\(\\text{AR}(1)\\) Multilevel Models in Intensive Longitudinal Designs",
    "section": "Conduct the sensitivity power analysis",
    "text": "Conduct the sensitivity power analysis\nWe can now conduct a sensitivity analysis to assess how power varies under different conditions. First, we focus on the scenario in which we varied the value of the main effect of interest.\n\nVarying the value of the main effect of interest\n\n# We investigate what happens with power when the main effect of interest is 10%\n# larger/smaller than the expected value based on previous studies.\n\n# Differences in the fixed autoregressive effect between the two groups.\nb11.Z.list &lt;- b11.Z * c(0.9, 1, 1.10)\n\n# Prepare the lists where we will store the results.\nPower.Simulation.list.REML.list &lt;- list()\nR.Converge.Simulation.list.REML.list &lt;- list()\n\n# Function for computing power using simulation-based approach using REML.\nfor (i in 1:length(b11.Z.list)) {\n    Power.Simulation.list.REML.list[[i]] &lt;- lapply(1:length(N.0), function(n) {\n        Power.Curve.Simulation.ML.AR.Group.Diff(\n            N.0[n],\n            N.1[n],\n            T.obs,\n            Ylag.center,\n            b00,\n            b01.Z,\n            b10,\n            b11.Z.list[i],\n            sigma,\n            sigma.v0,\n            sigma.v1,\n            rho.v,\n            alpha,\n            side.test,\n            Opt.Method = \"REML\",\n            R\n        )\n    })\n\n    # Number of replicates that converge.\n    R.Converge.Simulation.list.REML.list[[i]] &lt;- unlist(\n        lapply(1:length(N.0), function(n) {\n            Power.Simulation.list.REML.list[[i]][[n]]$n.R\n        })\n    )\n}\n\n# Print the number of replicates that converged.\nR.Converge.Simulation.list.REML.list\n\nWe plot the power curves for the sensitivity analysis for the value of the difference in NA between participants diagnosed with MDD and participants in the control group.\n\n# Construct a matrix with the results of the sensitivity power analysis.\nPower.b11.power &lt;- matrix(0, length(N.0), length(b11.Z.list))\n\n# Get the values of power.\nfor (i in 1:length(b11.Z.list)) {\n    Power.b11.power[, i] = unlist(\n        lapply(\n            1:length(N.0), function(n) {\n                Power.Simulation.list.REML.list[[i]][[n]]$power.hat.lme[\"b11.Z\"]\n            }\n        )\n    )\n}\n\n# Convert to long format.\nPower.b11.power = melt(Power.b11.power, id.vars = \"Power\")\n\n# Add column names.\ncolnames(Power.b11.power) &lt;- c(\"N.0\", \"Value\", \"Power\")\n\n# Store the values.\nPower.b11.power$N.0 &lt;- rep(N.0, length(b11.Z.list))\nPower.b11.power$N.1 &lt;- rep(N.1, length(b11.Z.list))\nPower.b11.power$Value &lt;- rep(b11.Z.list, each = length(N.0))\nPower.b11.power &lt;- Power.b11.power[, c(\"N.0\", \"N.1\", \"Value\", \"Power\")]\n\n# Convert to data frame.\nPower.b11.power &lt;- data.frame(Power.b11.power)\n\n# Create curve with the results of the sensitivity power analysis.\nggplot(Power.b11.power, aes(\n        x = N.0,\n        y = Power,\n        colour = as.factor(Value),\n        group = as.factor(Value)\n    )) +\n    geom_line(\n        size = 1\n    ) +\n    geom_point(\n        size = 1\n    ) +\n    labs(\n        title = \"Power curves for the difference in\n                the inertia of NA between participants \\n\n                diagnosed with MDD and participants in the\n                control group\",\n        x = \"Number of Persons\",\n        color = \"Difference in \\n\n                inertia of NA\"\n    ) +\n    scale_x_continuous(\n        name = \"Participants\",\n        breaks = N.0,\n        labels = c(paste(N.0, N.1, sep = \";\"))\n    ) +\n    theme(\n        axis.text.x = element_text(angle = 90)\n    ) +\n    scale_colour_manual(\n        values = c(\"#CC79A7\", \"#56B4E9\", \"#009E73\")\n    ) +\n    geom_hline(\n        yintercept = 0.9, linetype = \"dashed\"\n    ) +\n    theme(\n        legend.position = \"bottom\",\n        legend.title = element_text(size = 10)\n    ) +\n    theme_minimal()\n\nWe now conduct a sensitivity analysis to assess how power varies under different conditions. This time, we focus on the scenario in which we varied the average compliance rate.\n\n\nVarying the average compliance rate\n\n# Average compliance rate.\nT.obs.list &lt;- T.obs * c(0.6, 0.8, 1)\n\n# Prepare the lists where we will store the results.\nPower.Simulation.list.REML.Tpoints.list &lt;- list()\nR.Converge.Simulation.list.REML.Tpoints.list &lt;- list()\n\n# Function for computing power using simulation-based approach using REML.\nfor (i in 1:length(T.obs.list)) {\n    Power.Simulation.list.REML.Tpoints.list[[i]] &lt;- lapply(1:length(N.0), function(n) {\n        Power.Curve.Simulation.ML.AR.Group.Diff(\n            N.0[n],\n            N.1[n],\n            T.obs.list[i],\n            Ylag.center,\n            b00,\n            b01.Z,\n            b10,\n            b11.Z,\n            sigma,\n            sigma.v0,\n            sigma.v1,\n            rho.v,\n            alpha,\n            side.test,\n            Opt.Method = \"REML\",\n            R\n        )\n    })\n\n    # Number of replicates that converge.\n    R.Converge.Simulation.list.REML.Tpoints.list[[i]] &lt;- unlist(\n        lapply(\n            1:length(N.0), function(n) {\n                Power.Simulation.list.REML.Tpoints.list[[i]][[n]]$n.R\n            }\n        )\n    )\n}\n\n# Number of replicates that converge.\nR.Converge.Simulation.list.REML.Tpoints.list\n\nWe plot the power curves for the sensitivity analysis for the value of the difference in NA between participants diagnosed with MDD and participants in the control group.\n\n# Construct a matrix with the results of the sensitivity power analysis.\nPower.b11.power.Tpoints &lt;- matrix(0, length(N.0), length(T.obs.list))\n\n# Get the values of power.\nfor (i in 1:length(T.obs.list)) {\n    Power.b11.power.Tpoints[, i] &lt;- unlist(\n        lapply(\n            1:length(N.0), function(n) {\n                Power.Simulation.list.REML.Tpoints.list[[i]][[n]]$power.hat.lme[\"b11.Z\"]\n            }\n        )\n    )\n}\n\n# Transform to format for the plot.\nPower.b11.power.Tpoints &lt;- melt(Power.b11.power.Tpoints, id.vars = \"Power\")\n\n# Add column names.\ncolnames(Power.b11.power.Tpoints) &lt;- c(\"N.0\", \"Value\", \"Power\")\n\n# Store the values.\nPower.b11.power.Tpoints$N.0 &lt;- rep(N.0, length(T.obs.list))\nPower.b11.power.Tpoints$N.1 &lt;- rep(N.1, length(T.obs.list))\nPower.b11.power.Tpoints$Value &lt;- rep(T.obs.list, each = length(N.0))\nPower.b11.power.Tpoints &lt;- Power.b11.power.Tpoints[, c(\"N.0\", \"N.1\", \"Value\", \"Power\")]\n\n# Create curve with the results of the sensitivity power analysis.\nPower.b11.power.Tpoints &lt;- data.frame(Power.b11.power.Tpoints)\n\n# Create curve with the results of the sensitivity power analysis.\nggplot(Power.b11.power.Tpoints, aes(\n        x = N.0,\n        y = Power,\n        colour = as.factor(Value),\n        group = as.factor(Value)\n    )) +\n    geom_line(\n        size = 1\n    ) +\n    geom_point(\n        size = 1\n    ) +\n    labs(\n        title = \"Power curves for the difference in\n                the inertia of NA between participants \\n\n                diagnosed with MDD  and participants in\n                the control group\",\n        x = \"Number of Persons\",\n        color = \"Number of \\n time points\"\n    ) +\n    scale_x_continuous(\n        name = \"Participants\",\n        breaks = N.0,\n        labels = c(paste(N.0, N.1, sep = \";\"))\n    ) +\n    theme(\n        axis.text.x = element_text(angle = 90)\n    ) +\n    scale_colour_manual(\n        values = c(\"#CC79A7\", \"#56B4E9\", \"#009E73\")\n    ) +\n    geom_hline(\n        yintercept = 0.9,\n        linetype = \"dashed\"\n    ) +\n    theme(\n        legend.position = \"bottom\",\n        legend.title = element_text(size = 10)\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "exercises/power-multilevel-ar-intensive-designs.html#session-information",
    "href": "exercises/power-multilevel-ar-intensive-designs.html#session-information",
    "title": "Power Analysis for \\(\\text{AR}(1)\\) Multilevel Models in Intensive Longitudinal Designs",
    "section": "Session information",
    "text": "Session information\nUsing the command below, we can print the session information (i.e., operating system, details about the R installation, and so on) for reproducibility purposes.\n\n\n# Session information.\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.0    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.0       htmltools_0.5.5   rstudioapi_0.14   yaml_2.3.7       \n [9] rmarkdown_2.22    knitr_1.43        jsonlite_1.8.5    xfun_0.39        \n[13] digest_0.6.31     rlang_1.1.1       evaluate_0.21"
  },
  {
    "objectID": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html",
    "href": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html",
    "title": "Power Analysis for Multilevel Models With Cross-level Interactions in Intensive Longitudinal Designs",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ntidyverse\ngridExtra\nformattable\nhtmltools\nshiny\nDT\nggplot2\nplyr\ndplyr\ntidyr\nshinyjs\nshinythemes\nviridis\nplotly\nremotes\nnlme\ndevtools\ndata.table\nMASS\nfuture.apply\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\n    \"tidyverse\", \"gridExtra\", \"formattable\", \"htmltools\", \"shiny\",\n    \"DT\", \"ggplot2\", \"plyr\", \"dplyr\", \"tidyr\", \"shinyjs\", \"shinythemes\",\n    \"viridis\", \"plotly\", \"remotes\", \"nlme\", \"devtools\", \"data.table\",\n    \"MASS\", \"future.apply\"\n)\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nAt last, we can load the packages into our R session:\n\n# Load packages.\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(formattable)\nlibrary(htmltools)\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(shinyjs)\nlibrary(shinythemes)\nlibrary(viridis)\nlibrary(plotly)\nlibrary(remotes)\nlibrary(nlme)\nlibrary(devtools)\nlibrary(data.table)\nlibrary(MASS)\nlibrary(future.apply)"
  },
  {
    "objectID": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#setting-things-up",
    "href": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#setting-things-up",
    "title": "Power Analysis for Multilevel Models With Cross-level Interactions in Intensive Longitudinal Designs",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ntidyverse\ngridExtra\nformattable\nhtmltools\nshiny\nDT\nggplot2\nplyr\ndplyr\ntidyr\nshinyjs\nshinythemes\nviridis\nplotly\nremotes\nnlme\ndevtools\ndata.table\nMASS\nfuture.apply\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\n    \"tidyverse\", \"gridExtra\", \"formattable\", \"htmltools\", \"shiny\",\n    \"DT\", \"ggplot2\", \"plyr\", \"dplyr\", \"tidyr\", \"shinyjs\", \"shinythemes\",\n    \"viridis\", \"plotly\", \"remotes\", \"nlme\", \"devtools\", \"data.table\",\n    \"MASS\", \"future.apply\"\n)\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nAt last, we can load the packages into our R session:\n\n# Load packages.\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(formattable)\nlibrary(htmltools)\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(shinyjs)\nlibrary(shinythemes)\nlibrary(viridis)\nlibrary(plotly)\nlibrary(remotes)\nlibrary(nlme)\nlibrary(devtools)\nlibrary(data.table)\nlibrary(MASS)\nlibrary(future.apply)"
  },
  {
    "objectID": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#description",
    "href": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#description",
    "title": "Power Analysis for Multilevel Models With Cross-level Interactions in Intensive Longitudinal Designs",
    "section": "Description",
    "text": "Description\nThe goal of this exercise is to conduct a power analysis to select the number of persons to investigate if depression moderates the relationship between anhedonia and negative affect (NA).\nTo estimate this research question we will use a multilevel model with a cross-level interaction effect between a level 1 continuous predictor (anhedonia) and level 2 continuos predictor (depression):\n\\[\\begin{multline}\n    \\text{NA}_{it} = \\beta_{00} + \\beta_{01} \\text{Depression}_{i} + \\beta_{10} \\text{Anhedonia}_{it} +\\\\\n        \\beta_{01} \\text{Depression}_{i}\\text{Anhedonia}_{it} +\n        \\nu_{0i} + \\nu_{1i} \\text{Anhedonia}_{it}  + \\epsilon_{it}\n\\end{multline}\\]\nwhere \\(\\beta_{00}\\) is the fixed intercept, \\(\\beta_{01}\\) represents the main effect of depression, \\(\\beta_{10}\\) is the fixed slope of anhedonia, and \\(\\beta_{11}\\) represents the cross-level interaction effect between anhedonia and depression. The cross-level interaction effect assesses whether depression moderates the effect of anhedonia on NA. The level 1 predictor (i.e., anhedonia) is centered within-persons and within-days.\nIn this model, we account for the serial dependency that characterizes IL designs by assuming that the level 1 errors follow an autoregressive \\(\\text{AR}(1)\\) process (see Goldstein et al., 1994):\n\\[\n\\epsilon_{it} = \\rho_\\epsilon \\epsilon_{it-1} + \\varepsilon_{it}\n\\]\nwhere the correlation between two consecutive errors is denoted by \\(\\rho_\\epsilon\\), and \\(\\varepsilon_{it}\\) is a Gaussian error with mean zero and variance \\(\\sigma_\\epsilon^2\\). Under this model, the variance of the level 1 errors is given by \\(\\sigma_\\epsilon^2/(1-\\rho_\\epsilon^2)\\). To guarantee that this model is stationary, the autocorrelation \\(\\rho_\\epsilon\\) should range between -1 and 1 (Hamilton, 1994).\nBetween-person differences in the relation between anhedonia and NA are captured by including a random intercept \\(\\nu_{0i}\\) and random slope \\(\\nu_{1i}\\). These random effects are multivariate normal distributed with mean zero and covariance matrix \\(\\Sigma_\\nu\\):\n\\[\n\\boldsymbol{\\Sigma}_\\nu = \\begin{bmatrix}\n\\sigma_{\\nu_{0}}^2 & \\sigma_{\\nu_{01}}  \\\\\n\\sigma_{\\nu_{01}} & \\sigma_{\\nu_{1}}^2\n\\end{bmatrix}\n\\]\nIn this model, it is also assumed that the level 2 random effects and the Level 1 errors are independent.\nTo investigate if depression moderates the relationship between anhedonia and NA we are going to conduct the following hypothesis test:\n\\[\nH_0: \\beta_{11} = 0\n\\]\n\\[\nH_1: \\beta_{11} \\neq 0\n\\]\nThe aim of this exercise is to select the number of participants assuming the number of repeated measurements occasions to \\(T = 70\\).\n\nSelect sample size using the analytic approach, e.g., \\(N = \\{20, 40, ...\\}\\).\nCompare the results with the ones obtained using the simulation-based approach."
  },
  {
    "objectID": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#determining-model-parameter-values",
    "href": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#determining-model-parameter-values",
    "title": "Power Analysis for Multilevel Models With Cross-level Interactions in Intensive Longitudinal Designs",
    "section": "Determining model parameter values",
    "text": "Determining model parameter values\nTo obtain the values of the model parameters we will use data from the Leuven clinical study. The code to estimate the multilevel model with cross-level interaction effect is included in the exercise Multilevel model estimation using the Leuven Clinical Dataset. The output of the fitted model is:\n\n\n\nEstimated model parameters\n\n\nThus, the values of the model parameter that will be used to conduct the power analysis are:\n\\[\n\\begin{aligned}\n    \\beta_{00} &= 42.98 \\; \\; \\; \\text{fixed intercept} \\\\\n    \\beta_{01} &= 1.53 \\; \\; \\; \\text{main effect of depression} \\\\\n    \\beta_{10} &= 0.14 \\; \\; \\; \\text{fixed slope} \\\\\n    \\beta_{10} &= -0.01 \\; \\; \\; \\text{cross-level interaction effect} \\\\\n    \\sigma_\\epsilon &= 11.92 \\; \\; \\; \\text{std. deviation level 1 errors} \\\\\n    \\rho_\\epsilon &= 0.43 \\; \\; \\; \\text{std. deviation level 1 errors} \\\\\n    \\sigma_{\\nu_0} &= 12.86 \\; \\; \\; \\text{std. deviation random intercept} \\\\\n    \\sigma_{\\nu_1} &= 0.11 \\; \\; \\; \\text{std. deviation random slope} \\\\\n    \\rho_{\\nu_{01}} &= 0.249 \\; \\; \\; \\text{correlation between the random effects} \\\\\n    \\mu_\\text{Anhedonia} &= 51.66 \\; \\; \\; \\text{mean anhedonia} \\\\\n    \\sigma_\\text{Anhedonia} &= 23.67 \\; \\; \\; \\text{std. deviation anhedonia} \\\\\n    \\mu_\\text{Depression} &= 15.71 \\; \\; \\; \\text{mean anhedonia} \\\\\n    \\sigma_\\text{Depression} &= 5.00 \\; \\; \\; \\text{std. deviation anhedonia}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#analytical-based-power-analysis",
    "href": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#analytical-based-power-analysis",
    "title": "Power Analysis for Multilevel Models With Cross-level Interactions in Intensive Longitudinal Designs",
    "section": "Analytical-based power analysis",
    "text": "Analytical-based power analysis\nTo conduct the power analysis using the analytic approach we are going to use ApproxPowerIL: a Shiny application and R package to perform power analysis to select the number of persons for multilevel models with auto-correlated errors using asymptotic approximations of the information matrix The repository contains functions used in Lafit et al. (2023). Users can download the app and run locally on their computer by executing the following commands in R or RStudio at ApproxPowerIL.\n\n# Install the app package from the `GitHub` repository.\nremotes::install_github(\"ginettelafit/ApproxPowerIL\", force = TRUE)\n\n# Load package `ApproxPowerIL`.\nlibrary(ApproxPowerIL)\n\n# Lunch the app from the `GitHub` gist.\nshiny::runGist('302737dc046b89b7f09d15843389161c')\n\n\nStep 1\nSelect the model and set the sample size in the ApproxPowerIL application.\n\nIndicate the model of interest.\nInput the number of participants \\(N\\) (comma-separated): \\(N = \\{ 20, 40, 60, 80, 100 \\}\\).\nInput the number of repeated measurement occasions: \\(T = 70\\).\n\n\n\n\nModel and sample size selection in the Shiny application\n\n\n\n\nStep 2\nSet the value of the model parameters in the ApproxPowerIL application.\nUse the screenshots below to guide you through the process:\n\n\n\nModel parameters specification in the Shiny application\n\n\nAnd, for the remainder of the parameters:\n\n\n\nModel parameters specification in the Shiny application\n\n\n\n\nStep 3\nInspect the results.\nStatistical power is higher than 90% when the number of participants is equal to or higher than \\(20\\).\n\n\n\nPower curve for the analytical-based approach"
  },
  {
    "objectID": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#simulation-based-power-analysis",
    "href": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#simulation-based-power-analysis",
    "title": "Power Analysis for Multilevel Models With Cross-level Interactions in Intensive Longitudinal Designs",
    "section": "Simulation-based power analysis",
    "text": "Simulation-based power analysis\nTo conduct the power analysis using the simulation-based approach we are going to use PowerAnalysisIL: a Shiny application and R package to perform power analysis to select the number of persons for multilevel models using the simulation-based approach.\ndownload the app and run locally on their computer by executing the following The repository contains functions used in Lafit et al. (2021). Users can commands in R or RStudio at PowerAnalysisIL.\n\n# Install the app package from the `GitHub` repository.\nlibrary(devtools)\ndevtools::install_github(\"ginettelafit/PowerAnalysisIL\", force = TRUE)\n\n## Load package `PowerAnalysisIL`.\nlibrary(PowerAnalysisIL)\n\n# Lunch the app from the `GitHub` gist.\nshiny::runGist('6bac9d35c2521cc4fd91ce4b82490236')\n\n\nStep 1\nSelect the model and set the sample size in the PowerAnalysisIL application.\n\nIndicate the model of interest.\nInput the number of participants \\(N\\) (comma-separated): \\(N = \\{ 20, 40, 60, 80, 100 \\}\\).\nInput the number of repeated measurement occasions: \\(T = 70\\).\n\n\n\n\nModel and sample size selection in the Shiny application\n\n\n\n\nStep 2\nSet the value of the model parameters in the PowerAnalysisIL application.\nUse the screenshots below to guide you through the process:\n\n\n\nModel parameters specification in the Shiny application\n\n\nAnd, for the remainder of the parameters:\n\n\n\nModel parameters specification in the Shiny application\n\n\n\n\nStep 3\nInspect the results.\nStatistical power is higher than 90% when the number of participants is equal to or higher than \\(20\\).\n\n\n\nPower curve for the analytical-based approach"
  },
  {
    "objectID": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#session-information",
    "href": "exercises/power-multilevel-cross-level-interaction-intensive-designs.html#session-information",
    "title": "Power Analysis for Multilevel Models With Cross-level Interactions in Intensive Longitudinal Designs",
    "section": "Session information",
    "text": "Session information\nUsing the command below, we can print the session information (i.e., operating system, details about the R installation, and so on) for reproducibility purposes.\n\n\n# Session information.\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.0    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.0       htmltools_0.5.5   rstudioapi_0.14   yaml_2.3.7       \n [9] rmarkdown_2.22    knitr_1.43        jsonlite_1.8.5    xfun_0.39        \n[13] digest_0.6.31     rlang_1.1.1       evaluate_0.21"
  },
  {
    "objectID": "exercises/power-simulation-n-1-intensive-designs.html",
    "href": "exercises/power-simulation-n-1-intensive-designs.html",
    "title": "Simulation-based power analysis for \\(N = 1\\) intensive longitudinal designs",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ndata.table\npsych\ntidyverse\nMASS\nstringr\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\"data.table\", \"psych\", \"tidyverse\", \"MASS\", \"stringr\")\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nNow that we have all packages installed, we continue by loading them.\n\n# To create lagged outcome variables.\nlibrary(data.table)\n\n# To compute descriptive statistics.\nlibrary(psych)\n\n# A useful package.\nlibrary(tidyverse)\n\n# Handy functions for data analysis.\nlibrary(MASS)\n\n# For handy string manipulation.\nlibrary(stringr)\n\n# Set a seed for reproducibility.\nset.seed(123)"
  },
  {
    "objectID": "exercises/power-simulation-n-1-intensive-designs.html#settings-things-up",
    "href": "exercises/power-simulation-n-1-intensive-designs.html#settings-things-up",
    "title": "Simulation-based power analysis for \\(N = 1\\) intensive longitudinal designs",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ndata.table\npsych\ntidyverse\nMASS\nstringr\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\"data.table\", \"psych\", \"tidyverse\", \"MASS\", \"stringr\")\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nNow that we have all packages installed, we continue by loading them.\n\n# To create lagged outcome variables.\nlibrary(data.table)\n\n# To compute descriptive statistics.\nlibrary(psych)\n\n# A useful package.\nlibrary(tidyverse)\n\n# Handy functions for data analysis.\nlibrary(MASS)\n\n# For handy string manipulation.\nlibrary(stringr)\n\n# Set a seed for reproducibility.\nset.seed(123)"
  },
  {
    "objectID": "exercises/power-simulation-n-1-intensive-designs.html#preparing-the-simulation-functions",
    "href": "exercises/power-simulation-n-1-intensive-designs.html#preparing-the-simulation-functions",
    "title": "Simulation-based power analysis for \\(N = 1\\) intensive longitudinal designs",
    "section": "Preparing the simulation functions",
    "text": "Preparing the simulation functions\nWe need two functions to conduct a simulation-based power analysis.\n\none function that generates the datasets for the simulation\nand a second function that runs the Monte Carlo simulation to estimate the empirical power\n\n\nThe data generating function\nThis function generates a dataset from an \\(\\text{VAR}(1)\\) model and takes a few arguments as follows:\n\nvars is the number of variables of the \\(\\text{VAR}(1)\\) model\nTobs is the number of repeated measurements\ndelta the intercept matrix\npsi the transition matrix (i.e., containing the auto-regressive and cross-regressive effects)\nsigma the variance-covariance matrix of the innovation\n\n\n# Function to generate data from a `VAR(1)` model.\nsim_VAR_data &lt;- function(vars, Tobs, delta, psi, sigma) {\n    # Number of burning observations.\n    T.burning &lt;- 10000\n\n    # Define the number of observations.\n    T.total &lt;- T.burning + Tobs\n\n    # Simulate errors.\n    if (vars == 1) {\n        E &lt;- as.matrix(rnorm(T.total, 0, sigma))\n    } else {\n        E &lt;- mvrnorm(T.total, mu = rep(0, vars), sigma)\n    }\n\n    # Recursive equation.\n    Y &lt;- matrix(0, T.total, vars)\n\n    # Initialized values.\n    Y[1, ] &lt;- delta + E[1, ]\n\n    # Simulate dependent variables.\n    for (t in 2:T.total) {\n        Y[t, ] &lt;- delta + psi %*% Y[t - 1, ] + E[t, ]\n    }\n\n    # Exclude burning observations.\n    Y &lt;- Y[-seq(1:T.burning), ]\n\n    # Create lagged variables.\n    Y &lt;- cbind(Y, lag(Y))\n\n    # Rename variables.\n    colnames(Y) &lt;- c(sprintf(\"Y%d\", seq(1:vars)), sprintf(\"Y%dlag\", seq(1:vars)))\n\n    # Return the data.\n    return(as.data.frame(Y))\n}\n\n\n\nThe Monte Carlo simulation function\nThis function conducts the Monte Carlo simulation for a set of sample sizes (i.e., several different number of observations) and computes the statistical power for a given hypothesis. It takes several arguments as follows:\n\nvars is the number of variables of the \\(\\text{VAR}(1)\\) model\nTobs_list is a list of numbers of repeated measurements (i.e., Tobs)\ndelta the intercept matrix\npsi the transition matrix (which contains the auto-regressive and cross-regressive effects)\nsigma the variance-covariance matrix of the innovation\nR is the number of Monte Carlo replicates (e.g., \\(1000\\))\nalpha is the Type I error rate (or significance level of a test statistic)\n\n\n# Function to conduct the Monte Carlo power simulation.\nmc_power &lt;- function(vars, Tobs_list, delta, psi, sigma, R, alpha) {\n    # Prepare simulation storage.\n    df_pow &lt;- data.frame()\n\n    # For each sample size in the list.\n    for (i in 1:length(Tobs_list)) {\n        # Extract the sample size.\n        Tobs &lt;- Tobs_list[i]\n\n        # Print the progress.\n        print(paste0(\"Power analysis for N = \", Tobs))\n\n        # For each Monte Carlo replication.\n        for (r in 1:R) {\n            # Generate data.\n            data &lt;- sim_VAR_data(vars, Tobs, delta, psi, sigma)\n\n            # Create names lists.\n            var_names &lt;- sprintf(\"Y%d\", seq(1:vars))\n            lag_names &lt;- sprintf(\"Y%dlag\", seq(1:vars))\n\n            # Prepare storage for the model fits.\n            list_pow_rep &lt;- list()\n\n            # For each variable in the model.\n            for (nbName in 1:length(var_names)) {\n                # Extract variable name.\n                name_ &lt;- var_names[nbName]\n\n                # Create the formula for the linear regression.\n                formula &lt;- as.formula(\n                    paste(name_, paste(lag_names, collapse = \" + \"), sep = \" ~ \")\n                )\n\n                # Estimate model.\n                model &lt;- lm(formula, data)\n\n                # Extract the coefficients.\n                coefficients &lt;- summary(model)$coefficients\n\n                # Extract the `p` values.\n                df_pval &lt;- as.data.frame(rbind(coefficients[, 4]))\n\n                # Compute the empirical power.\n                list_pow_rep[[nbName]] &lt;- as.data.frame(df_pval &lt; alpha)\n\n                # Add names to the columns.\n                names(list_pow_rep[[nbName]]) &lt;- c(paste0(\"pow_int_\", name_), paste0(\"pow_\", lag_names, \"_\", name_))\n            }\n\n            # Bind the columns.\n            rep_data &lt;- bind_cols(\n                data.frame(Tobs = Tobs), do.call(cbind, list_pow_rep)\n            )\n\n            # Bind the rows.\n            df_pow &lt;- rbind(df_pow, rep_data)\n        }\n    }\n\n    # Compute empirical power.\n    df_pow &lt;- aggregate(\n        df_pow[, 2:ncol(df_pow)], list(df_pow$Tobs), mean\n    )\n\n    # Rename the columns.\n    df_pow &lt;- rename(df_pow, Tobs = Group.1)\n\n    return(df_pow)\n}"
  },
  {
    "objectID": "exercises/power-simulation-n-1-intensive-designs.html#conducting-the-simulation-power-analysis",
    "href": "exercises/power-simulation-n-1-intensive-designs.html#conducting-the-simulation-power-analysis",
    "title": "Simulation-based power analysis for \\(N = 1\\) intensive longitudinal designs",
    "section": "Conducting the simulation power analysis",
    "text": "Conducting the simulation power analysis\nTo conduct the simulation-based power analysis you first need to set the following arguments:\n\nTobs: list of numbers of repeated measurements.\nvars: number of variables of the \\(\\text{VAR}(1)\\) model.\ndelta: a \\(\\text{vars} \\times 1\\) matrix with one intercept per variable.\npsi: a \\(\\text{vars} \\times \\text{vars}\\) matrix. The diagonal elements are the autoregressive coefficients, and the off-diagonal are the cross-regressive coefficients.\nsigma: a \\(\\text{vars} \\times \\text{vars}\\) matrix. The diagonal elements are the variance of the residuals, and the off-diagonal elements are the covariance values. Note that the matrix should be symmetric.\nalpha: type I error rate (i.e., conventionally set at \\(.05\\)).\nR: the number of Monte Carlo replicates (i.e., \\(1000\\)).\npow_target: the empirical power targeted, which is often \\(.8\\). This variable is only used in the results (not in the simulation).\n\nFor the reminder of this document, we demonstrate how to run a simulation-based power analysis for the \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models.\n\nSimulation for the \\(\\text{AR}(1)\\) model\nWe ran a simulation for an \\(\\text{AR}(1)\\) model with \\(\\text{Tobs} = \\{50, 100, 150\\}\\), specified as follows:\n\\[\ny_{t} = 3 + .3 * y_{t-1} + \\varepsilon\n\\]\nwith:\n\\[\n\\varepsilon \\sim N(0, 10)\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nIn the example below we set the number of Monte Carlo replicates to R = 10 to speed up the computation and the generation of this document. In practice, you should set at lest R = 1000 to obtain reliable results. Make sure to change it accordingly when you run the code.\n\n\n\n# Specify the simulation inputs.\nTobs_list &lt;- c(50, 100, 150)\nvars &lt;- 1\ndelta &lt;- as.matrix(3)\npsi &lt;- as.matrix(.3)\nsigma &lt;- as.matrix(10)\nalpha &lt;- 0.05\nR &lt;- 10\npow_target &lt;- .8\n\n# Run the power analysis simulation.\ndf_pow_result &lt;- mc_power(vars, Tobs_list, delta, psi, sigma, R, alpha)\n\n[1] \"Power analysis for N = 50\"\n[1] \"Power analysis for N = 100\"\n[1] \"Power analysis for N = 150\"\n\n\nWith the simulation completed, we can continue to display the results of the power analysis in a plot and a recapitulation table.\n\n# Prepare the results data for the plot.\ndt &lt;- df_pow_result %&gt;%\n    gather(Coef_power, power, starts_with(\"pow\"))\n\n# Rename.\ndt$Coef_power = stringr::str_replace_all(\n    dt$Coef_power, c(\"pow_\" = \"\", \"_\" = \" -&gt; \")\n)\n\n# Plot the results of the `AR(1)` power analysis.\nggplot(dt,\n    aes(\n        y = power,\n        x = Tobs,\n        color = Coef_power\n    )) +\n    geom_line() +\n    geom_hline(\n        yintercept = pow_target,\n        color = \"red\",\n        linetype = 2\n    ) +\n    scale_y_continuous(\n        breaks = seq(0, 1, by = .1),\n        limits = c(0, 1)\n    ) +\n    labs(\n        y = \"Power\",\n        x = \"Time points\"\n    ) +\n    theme_classic()\n\n\n\n\n\n\n\n\nFinally, we can display the results in a table as shown below.\n\n# Create table of outputs.\ndt = df_pow_result %&gt;%\n    gather(pow, value, starts_with(\"pow\")) %&gt;%\n    spread(Tobs, value)\n\n# Rename variables.\ndt$pow &lt;- stringr::str_replace_all(dt$pow, c(\"pow_\"=\"\", \"_\"=\" -&gt; \"))\n\n# Rename the columns.\nnames(dt)[1] &lt;- \"Coefficients\"\n\n# Print the table.\nprint(dt)\n\n  Coefficients  50 100 150\n1    int -&gt; Y1 0.6 1.0 0.8\n2  Y1lag -&gt; Y1 0.7 0.9 0.8\n\n\n\n\nSimulation for the \\(\\text{VAR}(1)\\) model\nWe ran a simulation for a \\(\\text{VAR}(1)\\) model with three variables and \\(\\text{Tobs} = \\{50, 100, 150\\}\\), specified as follows:\n\\[\n    \\begin{bmatrix} y_{1t} \\\\ y_{2t} \\\\ y_{3t} \\end{bmatrix} =\n    \\begin{bmatrix} 4 \\\\ 6 \\\\ 10 \\end{bmatrix} +\n    \\begin{bmatrix} 0.5 \\ 0.14 \\ .2 \\\\ 0.1 \\ 0.4 \\ .03 \\\\ 0.05 \\ 0.12 \\ .6 \\end{bmatrix}\n    \\begin{bmatrix} y_{1, t-1} \\\\ y_{2, t-1} \\\\ y_{3, t-1} \\end{bmatrix} +\n    \\begin{bmatrix} \\varepsilon_{1t} \\\\ \\varepsilon_{2t} \\\\ \\varepsilon_{3t} \\end{bmatrix}\n\\]\nwith:\n\\[\n    \\begin{bmatrix} \\varepsilon_{1t} \\\\ \\varepsilon_{2t} \\\\ \\varepsilon_{3t} \\end{bmatrix} \\sim\n    N \\Bigg( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} , \\begin{bmatrix} 15 \\ 3 \\ 6 \\\\ 3 \\ 20 \\ 9 \\\\ 6 \\ 9 \\ 18  \\end{bmatrix} \\Bigg)\n\\]\nLet‚Äôs start by setting the values for the simulation. Check out the documentation for the mc_power function for more details on the inputs.\n\n\n\n\n\n\nImportant\n\n\n\nJust like before, in the example below we set the number of Monte Carlo replicates to R = 10 to speed up the computation and the generation of this document. In practice, you should set at lest R = 1000 to obtain reliable results. Make sure to change it accordingly when you run the code.\n\n\n\n# Set the values for conducting the power analysis.\nTobs_list = c(50, 100, 150)\nvars = 3\ndelta = as.matrix(c(4, 6, 10))\npsi = rbind(\n    c(.50, .14, .20),\n    c(.10, .40, .03),\n    c(.05, .12, .50)\n)\nsigma = rbind(\n    c(15,  3,  6),\n    c( 3, 20,  9),\n    c( 6,  9, 18)\n)\nalpha = 0.05\nR = 10\npow_target = .8\n\n# Run the power analysis simulation.\ndf_pow_result &lt;- mc_power(vars, Tobs_list, delta, psi, sigma, R, alpha)\n\n[1] \"Power analysis for N = 50\"\n[1] \"Power analysis for N = 100\"\n[1] \"Power analysis for N = 150\"\n\n\nWe can continue to display the results of the power analysis in a plot and then summarize them in a table.\n\n# Prepare the results data for the plot.\ndt = df_pow_result %&gt;%\n    gather(\n        Coef_power, power, starts_with(\"pow\")\n    )\n\n# Rename the variables.\ndt$Coef_power = stringr::str_replace_all(\n    dt$Coef_power, c(\"pow_\" = \"\", \"_\" = \" -&gt; \")\n)\n\n# Plot the results of the power analysis.\nggplot(dt,\n    aes(\n        y = power,\n        x = Tobs,\n        color = Coef_power\n    )) +\n    geom_line() +\n    geom_hline(\n        yintercept = pow_target,\n        color = \"red\",\n        linetype = 2\n    ) +\n    scale_y_continuous(\n        breaks = seq(0, 1, by = .1),\n        limits = c(0, 1)\n    ) +\n    labs(\n        y = \"Power\",\n        x = \"Time points\"\n    ) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n# Create table of outputs.\ndt = df_pow_result %&gt;%\n    gather(\n        pow,\n        value,\n        starts_with(\"pow\")\n    ) %&gt;%\n    spread(\n        Tobs,\n        value\n    )\n\n# Rename the variables.\ndt$pow &lt;- stringr::str_replace_all(\n    dt$pow, c(\"pow_\" = \"\", \"_\" = \" -&gt; \")\n)\n\n# Rename the columns.\nnames(dt)[1] &lt;- \"Coefficients\"\n\n# Print the table.\nprint(dt)\n\n   Coefficients  50 100 150\n1     int -&gt; Y1 0.3 0.7 0.9\n2     int -&gt; Y2 0.5 0.9 1.0\n3     int -&gt; Y3 1.0 1.0 1.0\n4   Y1lag -&gt; Y1 0.9 1.0 1.0\n5   Y1lag -&gt; Y2 0.0 0.2 0.2\n6   Y1lag -&gt; Y3 0.0 0.0 0.1\n7   Y2lag -&gt; Y1 0.1 0.4 0.6\n8   Y2lag -&gt; Y2 0.5 0.9 1.0\n9   Y2lag -&gt; Y3 0.2 0.1 0.6\n10  Y3lag -&gt; Y1 0.3 0.4 0.7\n11  Y3lag -&gt; Y2 0.1 0.1 0.0\n12  Y3lag -&gt; Y3 0.8 1.0 1.0"
  },
  {
    "objectID": "exercises/power-simulation-n-1-intensive-designs.html#session-information",
    "href": "exercises/power-simulation-n-1-intensive-designs.html#session-information",
    "title": "Simulation-based power analysis for \\(N = 1\\) intensive longitudinal designs",
    "section": "Session information",
    "text": "Session information\nUsing the command below, we can print the session information (i.e., operating system, details about the R installation, and so on) for reproducibility purposes.\n\n\n# Session information.\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-58.4     lubridate_1.9.2   forcats_1.0.0     stringr_1.5.0    \n [5] dplyr_1.1.2       purrr_1.0.1       readr_2.1.4       tidyr_1.3.0      \n [9] tibble_3.2.1      ggplot2_3.4.2     tidyverse_2.0.0   psych_2.3.3      \n[13] data.table_1.14.8\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      jsonlite_1.8.5    compiler_4.3.0    tidyselect_1.2.0 \n [5] parallel_4.3.0    scales_1.2.1      yaml_2.3.7        fastmap_1.1.1    \n [9] lattice_0.21-8    R6_2.5.1          labeling_0.4.2    generics_0.1.3   \n[13] knitr_1.43        htmlwidgets_1.6.2 munsell_0.5.0     tzdb_0.4.0       \n[17] pillar_1.9.0      rlang_1.1.1       utf8_1.2.3        stringi_1.7.12   \n[21] xfun_0.39         timechange_0.2.0  cli_3.6.1         withr_2.5.0      \n[25] magrittr_2.0.3    digest_0.6.31     grid_4.3.0        rstudioapi_0.14  \n[29] hms_1.1.3         lifecycle_1.0.3   nlme_3.1-162      vctrs_0.6.2      \n[33] mnormt_2.1.1      evaluate_0.21     glue_1.6.2        farver_2.1.1     \n[37] fansi_1.0.4       colorspace_2.1-0  rmarkdown_2.22    tools_4.3.0      \n[41] pkgconfig_2.0.3   htmltools_0.5.5"
  },
  {
    "objectID": "exercises/sample-size-analysis-powerly.html",
    "href": "exercises/sample-size-analysis-powerly.html",
    "title": "Sample size analysis using powerly",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ndata.table\npsych\ntidyverse\nMASS\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\"data.table\", \"psych\", \"tidyverse\", \"MASS\")\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nNow that we have all packages installed, we continue by loading them.\n\n# To create lagged outcome variables.\nlibrary(data.table)\n\n# To compute descriptive statistics.\nlibrary(psych)\n\n# A useful package.\nlibrary(tidyverse)\n\n# Handy functions for data analysis.\nlibrary(MASS)\n\n# Set a seed for reproducibility.\nset.seed(123)"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#settings-things-up",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#settings-things-up",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "",
    "text": "Before we proceed, we need to ensure we have several packages installed and loaded into our R session. For the scripts below, we will use the following packages:\n\ndata.table\npsych\ntidyverse\nMASS\n\nWhich we can install in one go as follows:\n\n# Prepare the package list.\npackages = c(\"data.table\", \"psych\", \"tidyverse\", \"MASS\")\n\n# Install packages.\ninstall.packages(packages)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may consider first checking if the packages are installed before actually installing them. Nevertheless, the code above will not reinstall packages that are already installed and up-to-date.\n\n\nNow that we have all packages installed, we continue by loading them.\n\n# To create lagged outcome variables.\nlibrary(data.table)\n\n# To compute descriptive statistics.\nlibrary(psych)\n\n# A useful package.\nlibrary(tidyverse)\n\n# Handy functions for data analysis.\nlibrary(MASS)\n\n# Set a seed for reproducibility.\nset.seed(123)"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#description",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#description",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Description",
    "text": "Description\nIn this tutorial, we use data from Heininga et al. (2019). In this study, the authors applied the ESM methodology to study emotion dynamics in people with Major Depressive Disorder (MDD). The study consist of an ESM testing period of seven days in which participants had to fill out questions about mood and social context on their daily lives ten times a day (i.e., \\(70\\) measurement occasions). The data set contains \\(38\\) participants diagnosed with MDD and \\(40\\) control subjects. Participants filled out the ESM questionnaires in a stratified random interval scheme between 9:30 AM and 9:30 PM.\nFirst, we are going to load the data set:\n\n# Load the data set.\nload(file = \"assets/data/clinical-dataset.RData\")\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure you load the data from the location where you downloaded it. If your analysis script (i.e., the .R file) and the dataset are in the same location, than you can simply load the data as follows:\nload(file = \"clinical-dataset.RData\")"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#data-exploration",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#data-exploration",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Data exploration",
    "text": "Data exploration\nIn this section we will explore briefly the variables in the data set.\n\nData structure\nNow, that we have the data ready, we can start by exploring it to get a better understanding of the variable measured.\n\n# Select the first participant diagnosed with MDD.\ni.ID &lt;- unique(data$PID[data$MDD == 1])[1]\n\n# Select data from participant with person identification number `101`.\ndata &lt;- data[data$PID == 101, ]\n\n\n\n\n\n\n\nNote\n\n\n\nFrom now on we will work with data from participant 101 only. In other words, data is now a subset of the original data set, containing only the responses from participant 101.\n\n\n\n# Find the dimensions.\ndim(data)\n\n[1] 70  8\n\n# Find the structure.\nstr(data)\n\n'data.frame':   70 obs. of  8 variables:\n $ PID      : num  101 101 101 101 101 101 101 101 101 101 ...\n $ day      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ daybeep  : num  1 2 3 4 5 6 7 8 9 10 ...\n $ PA       : num  NA 27.3 49.7 43 43 ...\n $ NA.      : num  NA 30.4 23.8 24.2 32.8 19.6 18.4 21.2 23 21.8 ...\n $ anhedonia: num  NA 26 25 25 50 21 42 30 22 30 ...\n $ MDD      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ QIDS     : num  12 12 12 12 12 12 12 12 12 12 ...\n\n# See the first 6 rows.\nhead(data)\n\n  PID day daybeep       PA  NA. anhedonia MDD QIDS\n1 101   1       1       NA   NA        NA   1   12\n2 101   1       2 27.33333 30.4        26   1   12\n3 101   1       3 49.66667 23.8        25   1   12\n4 101   1       4 43.00000 24.2        25   1   12\n5 101   1       5 43.00000 32.8        50   1   12\n6 101   1       6 18.00000 19.6        21   1   12\n\n# See the last 6 rows.\ntail(data)\n\n   PID day daybeep       PA  NA. anhedonia MDD QIDS\n65 101   7       5 24.33333 31.8        52   1   12\n66 101   7       6 28.66667 20.6        53   1   12\n67 101   7       7 23.33333 23.8        51   1   12\n68 101   7       8 33.66667 36.2        46   1   12\n69 101   7       9 41.66667 21.0        29   1   12\n70 101   7      10 34.00000 18.4        47   1   12\n\n# Find the column names.\nnames(data)\n\n[1] \"PID\"       \"day\"       \"daybeep\"   \"PA\"        \"NA.\"       \"anhedonia\"\n[7] \"MDD\"       \"QIDS\"     \n\n# Summary of the data.\nsummary(data)\n\n      PID           day       daybeep           PA             NA.       \n Min.   :101   Min.   :1   Min.   : 1.0   Min.   :14.67   Min.   :14.40  \n 1st Qu.:101   1st Qu.:2   1st Qu.: 3.0   1st Qu.:26.08   1st Qu.:21.40  \n Median :101   Median :4   Median : 5.5   Median :33.33   Median :25.90  \n Mean   :101   Mean   :4   Mean   : 5.5   Mean   :33.51   Mean   :29.62  \n 3rd Qu.:101   3rd Qu.:6   3rd Qu.: 8.0   3rd Qu.:41.67   3rd Qu.:36.25  \n Max.   :101   Max.   :7   Max.   :10.0   Max.   :57.33   Max.   :65.60  \n                                          NA's   :6       NA's   :6      \n   anhedonia          MDD         QIDS   \n Min.   :14.00   Min.   :1   Min.   :12  \n 1st Qu.:24.75   1st Qu.:1   1st Qu.:12  \n Median :41.50   Median :1   Median :12  \n Mean   :39.34   Mean   :1   Mean   :12  \n 3rd Qu.:52.00   3rd Qu.:1   3rd Qu.:12  \n Max.   :83.00   Max.   :1   Max.   :12  \n NA's   :6                               \n\n# Number of participants.\nlength(unique(data$PID))\n\n[1] 1\n\n\nThe data set contains the following variables:\n\nPID that denotes the individual identification number\nday is a variable that ranges from 1 to 7 and identifies the day of ESM testing\ndaybeep is a variable that ranges from 1 to 10 and identifies the number of the prompt or beep within a day\nPA is the Positive Affect computed as the mean of items:\n\nHow happy do you feel at the moment?\nHow relaxed do you feel at the moment?\nHow euphoric do you feel at the moment?\n\nNA. is the Negative Affect computed as the mean of items:\n\nHow depressed do you feel at the moment?\nHow stressed do you feel at the moment?\nHow anxious do you feel at the moment?\nHow angry do you feel at the moment?\nHow restless do you feel at the moment?\n\nanhedonia corresponds to the ESM item:\n\nTo what degree do you find it difficult to experience pleasure in activities at the moment?\n\nMDD is a dummy variable equal to one when the individual has been diagnosed with MDD and 0 otherwise\nQIDS denotes the sum of the items of the Quick Inventory of Depressive Symptomatology (QIDS; Rush et al., 2003). QIDS was measured before the ESM testing period.\n\n\n\n\n\n\n\nNote\n\n\n\nTime-varying variables (PA, NA, and anhedonia) have been lagged within days to account for the night breaks.\n\n\n\n\nDescriptive statistics and visualizations\nWe first obtain some descriptive statistics including number of observations per day, and compliance.\n\n# Get the number of assessment per day.\ntable(data$PID)\n\n\n101 \n 70 \n\n# Compute a binary variable indicating if a participant answered a beep. We take\n# the ESM item PA as reference because in this ESM design participants were not\n# allowed to skip items.\ndata$Compliance &lt;- ifelse(is.na(data$PA) == FALSE, 1, 0)\n\n# Mean, median of the compliance for the participant PID=101\ndescribe(data$Compliance)\n\n   vars  n mean   sd median trimmed mad min max range skew kurtosis   se\nX1    1 70 0.91 0.28      1       1   0   0   1     1 -2.9     6.48 0.03\n\n\nNext, we can obtain visualizations and statistics of the distribution of the person-level or time-invariant variables variables.\n\n# We create a data set that will aggregate the data by the time invariant\n# variables, i.e., the MDD diagnosis and QIDS depression score.\ndt.person = aggregate(\n    cbind(data$MDD, data$QIDS),\n    by = list(data$PID),\n    mean,\n    na.rm = TRUE\n)\n\n# Add column names to the aggregated data set.\ncolnames(dt.person) = c(\"Group.1\", \"MDD\", \"QIDS\")\n\n# Print the aggregated data set.\ndt.person\n\n  Group.1 MDD QIDS\n1     101   1   12\n\n\nWe now focus on time-varying variables NA, PA, and anhedonia and we obtain visualization and descriptive statistics\n\n# Histogram for the time-varying variable negative affect (NA.).\nggplot(data, aes(NA.)) +\n    geom_histogram(\n        color = \"black\",\n        fill = \"white\",\n        bins = 30\n    ) +\n    theme_bw()\n\n\n\n\n\n\n\n# Descriptive statistics for NA.\ndescribe(data$NA.)\n\n   vars  n  mean    sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 64 29.62 11.47   25.9   28.13 7.86 14.4 65.6  51.2 1.25     1.01 1.43\n\n# Create obs order variable.\ndata$obs = 1:nrow(data)\n\n# Plot the trajectories of the time-varying variable NA by person.\ndata %&gt;%\n    ggplot(aes(x = obs, y = NA.)) +\n    geom_point() +\n    geom_line() +\n    theme_bw()\n\n\n\n\n\n\n\n# Histogram for the time-varying variable negative affect (PA).\nggplot(data, aes(PA)) +\n    geom_histogram(\n        color = \"black\",\n        fill = \"white\",\n        bins = 30\n    ) +\n    theme_bw()\n\n\n\n\n\n\n\n# Descriptive statistics for PA.\ndescribe(data$PA)\n\n   vars  n  mean    sd median trimmed   mad   min   max range skew kurtosis\nX1    1 64 33.51 10.32  33.33   33.33 12.35 14.67 57.33 42.67 0.09    -0.84\n     se\nX1 1.29\n\n# Plot the trajectories of the time-varying variable PA by person.\ndata %&gt;%\n    ggplot(aes(x = obs, y = PA)) +\n    geom_point() +\n    geom_line() +\n    theme_bw()\n\n\n\n\n\n\n\n# Histogram for the time-varying variable anhedonia.\nggplot(data, aes(anhedonia)) +\n    geom_histogram(\n        color = \"black\",\n        fill = \"white\",\n        bins = 30\n    ) +\n    theme_bw()\n\n\n\n\n\n\n\n# Descriptive statistics for anhedonia.\ndescribe(data$anhedonia)\n\n   vars  n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 64 39.34 15.84   41.5   38.92 20.02  14  83    69 0.22    -0.77 1.98\n\n# Plot the trajectories of the time-varying variable anhedonia by person.\ndata %&gt;%\n    ggplot(aes(x = obs, y = anhedonia)) +\n    geom_point() +\n    geom_line() +\n    theme_bw()"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#data-preparation",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#data-preparation",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Data preparation",
    "text": "Data preparation\nAt this point, we are almost ready to start estimating our models. The last step before doing that, is creating the lagged version of the variables PA and NA.. They will be used on the subsequent \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models we fit below.\n\n# Create lagged variables.\n# Lagged within days to take into account night breaks.\ndata$PA.lag &lt;- rep(NA, nrow(data))\ndata$NA.lag &lt;- rep(NA, nrow(data))\ndata$anhedonia.lag &lt;- rep(NA, nrow(data))\nday.id &lt;- unique(data$day)\n\nfor (t in day.id) {\n    data$PA.lag[which(data$day == t)] &lt;- shift(data$PA[which(data$day == t)], 1)\n    data$NA.lag[which(data$day == t)] &lt;- shift(data$NA.[which(data$day == t)], 1)\n    data$anhedonia.lag[which(data$day == t)] &lt;- shift(data$anhedonia[which(data$day == t)], 1)\n}"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#estimating-textar1-for-pa",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#estimating-textar1-for-pa",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Estimating \\(\\text{AR}(1)\\) for PA",
    "text": "Estimating \\(\\text{AR}(1)\\) for PA\nWe estimate an \\(\\text{AR}(1)\\) model for PA using a linear regression model (ordinary least squares, OLS). You can extract the estimates with the summary() function. Finally, you can compute the estimate of the standard deviation of the errors of the \\(\\text{AR}(1)\\) model computing the standard deviation using the function sd() on the residuals of the fitted model.\n\n# Fit AR(1) model for PA.\nfit.AR.PA &lt;- lm(PA ~ 1 + PA.lag, data = data)\n\n# Show fit summary.\nsummary(fit.AR.PA)\n\n\nCall:\nlm(formula = PA ~ 1 + PA.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.486  -5.866   2.070   5.820  18.155 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.3258     4.5755   4.442 4.68e-05 ***\nPA.lag        0.4092     0.1308   3.130  0.00287 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.658 on 52 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1585,    Adjusted R-squared:  0.1423 \nF-statistic: 9.796 on 1 and 52 DF,  p-value: 0.002866\n\n# Estimate the standard deviation of the errors.\nsd(residuals(fit.AR.PA))\n\n[1] 9.566865"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#estimating-textvar1-for-pa-and-na",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#estimating-textvar1-for-pa-and-na",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Estimating \\(\\text{VAR}(1)\\) for PA and NA",
    "text": "Estimating \\(\\text{VAR}(1)\\) for PA and NA\nWe estimate a \\(\\text{VAR}(1)\\) model for PA and NA using two separate linear regression models. You can extract the estimates with the summary() function. Finally, you can compute the estimate of the variance-covariance matrix of the errors of the \\(\\text{VAR}(1)\\) model computing the covariance matrix using the function cov() on the residuals of each of the fitted models.\n\n# Linear regression model for PA.\nfit.VAR.PA = lm(PA ~ 1 + PA.lag + NA.lag, data = data)\n\n# Linear regression model for NA.\nfit.VAR.NA = lm(NA. ~ 1 + PA.lag + NA.lag, data = data)\n\n# Show fit summary for PA.\nsummary(fit.VAR.PA)\n\n\nCall:\nlm(formula = PA ~ 1 + PA.lag + NA.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.551  -6.480   1.262   5.859  18.008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.45530    6.27640   3.737 0.000471 ***\nPA.lag       0.39213    0.13341   2.939 0.004930 ** \nNA.lag      -0.08273    0.11299  -0.732 0.467416    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.702 on 51 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1673,    Adjusted R-squared:  0.1346 \nF-statistic: 5.123 on 2 and 51 DF,  p-value: 0.009391\n\n# Show fit summary for NA.\nsummary(fit.VAR.NA)\n\n\nCall:\nlm(formula = NA. ~ 1 + PA.lag + NA.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.260  -5.933  -2.073   3.104  39.302 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  17.8818     6.6337   2.696  0.00949 **\nPA.lag       -0.0202     0.1410  -0.143  0.88664   \nNA.lag        0.3756     0.1194   3.145  0.00277 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.25 on 51 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1692,    Adjusted R-squared:  0.1366 \nF-statistic: 5.194 on 2 and 51 DF,  p-value: 0.008851\n\n# Estimate variance-covariance matrix of the errors.\nres = cbind(residuals(fit.VAR.PA),residuals(fit.VAR.NA))\n\n# Print the covariance matrix.\ncov(res)\n\n          [,1]       [,2]\n[1,] 90.572865   4.295723\n[2,]  4.295723 101.177796"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#estimating-textvar1-model-for-na-pa-and-anhedonia",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#estimating-textvar1-model-for-na-pa-and-anhedonia",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Estimating \\(\\text{VAR}(1)\\) model for NA, PA and anhedonia",
    "text": "Estimating \\(\\text{VAR}(1)\\) model for NA, PA and anhedonia\nWe estimate a \\(\\text{VAR}(1)\\) model for PA, NA and anhedonia using three separate linear regression models. You can extract the estimates with the summary() function. Finally, you can compute the estimate of the variance-covariance matrix of the errors of the \\(\\text{VAR}(1)\\) model by computing the covariance matrix using the function cov() on the residuals of each of the fitted models.\n\n# Linear regression model for PA.\nfit.VAR.PA = lm(PA ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\n\n# Linear regression model for NA.\nfit.VAR.NA = lm(NA. ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\n\n# Linear regression model for anhedonia.\nfit.VAR.anhedonia = lm(anhedonia ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\n\n# Show fit summary for PA.\nsummary(fit.VAR.PA)\n\n\nCall:\nlm(formula = PA ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.374  -5.907   1.342   5.856  18.787 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   22.60373    6.50921   3.473  0.00107 **\nPA.lag         0.39067    0.13436   2.908  0.00542 **\nNA.lag        -0.12662    0.13926  -0.909  0.36759   \nanhedonia.lag  0.05564    0.10179   0.547  0.58711   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.769 on 50 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1722,    Adjusted R-squared:  0.1226 \nF-statistic: 3.468 on 3 and 50 DF,  p-value: 0.02288\n\n# Show fit summary for NA.\nsummary(fit.VAR.NA)\n\n\nCall:\nlm(formula = NA. ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.413  -5.302  -2.096   3.864  33.201 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   14.84269    6.66252   2.228   0.0304 *\nPA.lag        -0.02541    0.13752  -0.185   0.8542  \nNA.lag         0.21894    0.14254   1.536   0.1309  \nanhedonia.lag  0.19856    0.10419   1.906   0.0624 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.999 on 50 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.2255,    Adjusted R-squared:  0.179 \nF-statistic: 4.852 on 3 and 50 DF,  p-value: 0.00486\n\n# Show fit summary for anhedonia.\nsummary(fit.VAR.anhedonia)\n\n\nCall:\nlm(formula = anhedonia ~ 1 + PA.lag + NA.lag + anhedonia.lag, \n    data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.750  -9.599   1.195  10.194  29.364 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   14.94851    8.92402   1.675   0.1002  \nPA.lag         0.04386    0.18420   0.238   0.8128  \nNA.lag         0.32640    0.19093   1.710   0.0936 .\nanhedonia.lag  0.30771    0.13955   2.205   0.0321 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.39 on 50 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.268, Adjusted R-squared:  0.2241 \nF-statistic: 6.102 on 3 and 50 DF,  p-value: 0.001276\n\n# Estimate variance-covariance matrix of the errors.\nres = cbind(residuals(fit.VAR.PA),residuals(fit.VAR.NA),residuals(fit.VAR.anhedonia))\n\n# Print the covariance matrix.\ncov(res)\n\n          [,1]      [,2]      [,3]\n[1,] 90.034929  2.375884  16.02124\n[2,]  2.375884 94.326082  37.21901\n[3,] 16.021244 37.219012 169.22933"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#running-the-shiny-application",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#running-the-shiny-application",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Running the Shiny application",
    "text": "Running the Shiny application\nThe Shiny application is associated to a package that is stored at gitlab.kuleuven.be/ppw-okpiv/researchers/u0148925/shinyapp-paa_var_n1. To install the package and run the Shiny app, you can use the following R code:\n\n# Install the package containing the application.\nremotes::install_gitlab(\n    \"ppw-okpiv/researchers/u0148925/shinyapp-paa_var_n1\",\n    host = \"https://gitlab.kuleuven.be\",\n    force = TRUE\n)\n\n# Import the package containing the application.\nlibrary(paavar1)\n\n# Run the shiny app.\nrun_paa_var1()"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#power-analysis-for-textvar1-with-three-variables",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#power-analysis-for-textvar1-with-three-variables",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Power analysis for \\(\\text{VAR}(1)\\) with three variables",
    "text": "Power analysis for \\(\\text{VAR}(1)\\) with three variables\n\n\n\n\n\n\nExercise\n\n\n\nTry on your own and compare your results to the ones presented below!\n\n\n\nPower analysis result\nRunning the simulation with the application, you should end up with a similar plot as the one below.\n\n\n\nPower analysis \\(\\text{VAR}(1)\\) with three variables\n\n\n\n\nSensitivity analysis for power: varying parameters\nWe slightly changed the values of three coefficients to investigate how they change either the sample size recommendation or the precision of estimates:\n\n\\(\\beta_{11} = .39\\) to \\(\\beta_{11} = .8\\)\n\\(\\sigma_{00} = 90\\) to \\(\\sigma_{00} = 180\\)\n\\(R = 1000\\) to \\(R = 100\\)\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat conclusions can you draw based on the following power curves?\n\n\n\n\n\nSensitivity analysis for power\n\n\n\n\nSensitivity analysis for power: using CI\nFollowing Lafit, Revol et al.¬†(under review), we run a sensitivity analysis using the upper and lower boundaries of the estimated coefficients of interest. First, we extract the 95% confidence interval of the estimated values of each parameter.\n\n# Linear regression model for PA.\nconfint(fit.VAR.PA, level = 0.95)\n\n# Linear regression model for NA.\nconfint(fit.VAR.NA, level = 0.95)\n\n# Linear regression model for anhedonia.\nconfint(fit.VAR.anhedonia, level = 0.95)\n\nWe only varied the parameter values for the auto-regressive effect of PA (\\(\\beta_{11}\\)) following the confidence interval. We run two new power analyses. The results are displayed below.\n\n\n\nSensitivity analysis for power\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat conclusions can you draw based on the following power curves?"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#paa-for-textvar1-with-three-variables",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#paa-for-textvar1-with-three-variables",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "PAA for \\(\\text{VAR}(1)\\) with three variables",
    "text": "PAA for \\(\\text{VAR}(1)\\) with three variables\n\n\n\n\n\n\nExercise\n\n\n\nTry on your own and compare your results to the ones presented below!\n\n\n\nPAA result\nRunning the simulation with the application, you should end up with a similar plot as the one below.\n\n\n\nPAA for \\(\\text{VAR}(1)\\) with three variables\n\n\n\n\nSensitivity analysis for power: varying parameters\nWe changed the values of the transition matrix to investigate how it changes the sample size recommendation.\n\n\n\nSensitive PAA for \\(\\text{VAR}(1)\\)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat conclusions can you draw based on the following power curves?\n\n\n\n\n\n\n\n\nNote\n\n\n\nDespite the raising of the coefficients, the new transition matrix still fulfills the stationary assumption. Higher coefficients could lead to a violation of this assumption."
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1-intensive-designs.html#session-information",
    "href": "exercises/sample-size-solutions-n-1-intensive-designs.html#session-information",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Session information",
    "text": "Session information\nUsing the command below, we can print the session information (i.e., operating system, details about the R installation, and so on) for reproducibility purposes.\n\n\n# Session information.\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-58.4     lubridate_1.9.2   forcats_1.0.0     stringr_1.5.0    \n [5] dplyr_1.1.2       purrr_1.0.1       readr_2.1.4       tidyr_1.3.0      \n [9] tibble_3.2.1      ggplot2_3.4.2     tidyverse_2.0.0   psych_2.3.3      \n[13] data.table_1.14.8\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      jsonlite_1.8.5    compiler_4.3.0    tidyselect_1.2.0 \n [5] parallel_4.3.0    scales_1.2.1      yaml_2.3.7        fastmap_1.1.1    \n [9] lattice_0.21-8    R6_2.5.1          labeling_0.4.2    generics_0.1.3   \n[13] knitr_1.43        htmlwidgets_1.6.2 munsell_0.5.0     tzdb_0.4.0       \n[17] pillar_1.9.0      rlang_1.1.1       utf8_1.2.3        stringi_1.7.12   \n[21] xfun_0.39         timechange_0.2.0  cli_3.6.1         withr_2.5.0      \n[25] magrittr_2.0.3    digest_0.6.31     grid_4.3.0        rstudioapi_0.14  \n[29] hms_1.1.3         lifecycle_1.0.3   nlme_3.1-162      vctrs_0.6.2      \n[33] mnormt_2.1.1      evaluate_0.21     glue_1.6.2        farver_2.1.1     \n[37] fansi_1.0.4       colorspace_2.1-0  rmarkdown_2.22    tools_4.3.0      \n[41] pkgconfig_2.0.3   htmltools_0.5.5"
  },
  {
    "objectID": "slides/slides.html",
    "href": "slides/slides.html",
    "title": "Coming soon",
    "section": "",
    "text": "Coming soon."
  }
]