[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "",
    "text": "Workshop on Sample Size Planning for  Intensive Longitudinal Studies\nGinette Lafit, Jordan Revol, Mihai A. Constantin, & Eva Ceulemans"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìù Description",
    "text": "üìù Description\nIn recent years the popularity of procedures to collect intensive longitudinal data such as the Experience Sampling Method has increased immensely. The data collected using such designs allow researchers to study the dynamics of psychological processes, and how these dynamics differ across individuals. A fundamental question when designing a study is how to determine the sample size, which is closely related to the replicability and generalizability of empirical findings. Even though multiple statistical guidelines are available for sample size planning, it still remains a demanding enterprise in complex designs. The goal of this workshop is to address this crucial question by presenting methodological advances for sample size planning for intensive longitudinal designs. First, we provide an overview of methods for sample size planning with special emphasis on a priori power analysis. Second, we focus on how to conduct power analysis in the \\(N = 1\\) case when the goal is to model within-person processes using \\(\\text{VAR}(1)\\) models. Subsequently, we consider the extension to multilevel data in which multiple individuals are measured over time. We introduce an approach for conducting power analysis for multilevel models that explicitly accounts for the temporal dependencies that characterize the data collected in IL studies. In addition, we showcase how to perform power analysis for these models using a user-friendly and open-source application. Finally, we consider an alternative criterion for conducting sample size planning that targets the predictive accuracy of a model for unseen data. Focusing on \\(\\text{VAR}(1)\\) models in an \\(N = 1\\) context, we introduce a novel approach, called predictive accuracy analysis, to assess how many measurement occasions are required in order to optimize predictive accuracy."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìä Learning Objectives",
    "text": "üìä Learning Objectives\nThe workshop provides a road map on how to determine the sample size in IL designs.\nUpon course completion, participants will:\n\nbe familiar with methods for conducting power analysis for \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models in \\(N = 1\\) intensive logitudinal designs\nunderstand the different steps of the simulation-based power analysis approach\nbe able to implement existing tool for conducting power analysis for \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models in \\(N = 1\\) intensive logitudinal designs\nbe familiar with an new methods for conducting sample size analysis based on criteria different than statistical power (e.g., predictive accuracy or sensitivity)"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üíª Prerequisites",
    "text": "üíª Prerequisites\nParticipants should have some basic knowledge of R and some experience with RStudio. For the hands-on parts of the workshop, you need to install R version 4.1.2 or higher, RStudio, and the following R packages:\n\ndata.table\npsych\nggplot2\ntidyverse\npowerly"
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìÇ Modules",
    "text": "üìÇ Modules\n\n\n\n\n\n\nTo be updated\n\n\n\nWe should update the modules to reflect the program changes.\n\n\n\n\n\nTopic\nDuration\nSlides\nTutorial\n\n\n\n\nSample Size Planning\n0.0h\nlink\nlink\n\n\n\\(N = 1\\): Introduction\n0.0h\nlink\nlink\n\n\n\\(N = 1\\): Predictive Accuracy\n0.0h\nlink\nlink\n\n\nMultilevel models\n0.0h\nlink\nlink\n\n\nAdvanced methods\n0.0h\nlink\nlink\n\n\nTake home\n0.0h\nlink\nlink"
  },
  {
    "objectID": "index.html#given-at",
    "href": "index.html#given-at",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "üìç Given At",
    "text": "üìç Given At\n\n\n\nConference\nLocation\nDate\nLink\n\n\n\n\nSAA 2023\nAmsterdam, The Netherlands\nJune 8th, 2023\nlink"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "‚úçÔ∏è Citation",
    "text": "‚úçÔ∏è Citation\n\nLafit, G., Revol, J., Constantin M. A., & Ceulemans, E. (2023). Workshop on Sample Size Planning for Intensive Longitudinal Studies. https://00.0000/zenodo.0000000"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "\nWorkshop on Sample Size Planning for  Intensive Longitudinal Studies\n",
    "section": "‚öñÔ∏è License",
    "text": "‚öñÔ∏è License\n\n\nThe scripts, slides, and other materials by Ginette Lafit, Jordan Revol, Mihai A. Constantin, and Eva Ceulemans are licensed under CC BY 4.0  ."
  },
  {
    "objectID": "presenters/eva-ceulemans.html",
    "href": "presenters/eva-ceulemans.html",
    "title": "Eva Ceulemans",
    "section": "",
    "text": "Professor of Quantitative Psychology and Individual Differences at KU Leuven"
  },
  {
    "objectID": "presenters/eva-ceulemans.html#interests",
    "href": "presenters/eva-ceulemans.html#interests",
    "title": "Eva Ceulemans",
    "section": "üîç Interests",
    "text": "üîç Interests\nDimension reduction methods for multiset and multiway data\nTime series analysis\nChange point detection methods\nAnalysis of dyadic data\nModeling of emotion dynamics"
  },
  {
    "objectID": "presenters/ginette-lafit.html",
    "href": "presenters/ginette-lafit.html",
    "title": "Ginette Lafit",
    "section": "",
    "text": "PostDoc at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences and Center of Contextual Psychiatry"
  },
  {
    "objectID": "presenters/ginette-lafit.html#section",
    "href": "presenters/ginette-lafit.html#section",
    "title": "Ginette Lafit",
    "section": "",
    "text": "PostDoc at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences and Center of Contextual Psychiatry"
  },
  {
    "objectID": "presenters/ginette-lafit.html#interests",
    "href": "presenters/ginette-lafit.html#interests",
    "title": "Ginette Lafit",
    "section": "üîç Interests",
    "text": "üîç Interests\nStatistical methods for intensive longitudinal designs\nMethodology & open science in ESM research\nModel selection and regularization\nDimension reduction models"
  },
  {
    "objectID": "presenters/jordan-revol.html",
    "href": "presenters/jordan-revol.html",
    "title": "Jordan Revol",
    "section": "",
    "text": "PostDoc at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences"
  },
  {
    "objectID": "presenters/jordan-revol.html#section",
    "href": "presenters/jordan-revol.html#section",
    "title": "Jordan Revol",
    "section": "",
    "text": "PostDoc at KU Leuven in the Research Group of Quantitative Psychology and Individual Differences"
  },
  {
    "objectID": "presenters/jordan-revol.html#interests",
    "href": "presenters/jordan-revol.html#interests",
    "title": "Jordan Revol",
    "section": "üîç Interests",
    "text": "üîç Interests\nStatistical methods for intensive longitudinal designs\nSample size planning methods and predictive accuracy in intensive longitudinal studies"
  },
  {
    "objectID": "presenters/mihai-constantin.html",
    "href": "presenters/mihai-constantin.html",
    "title": "Mihai Constantin",
    "section": "",
    "text": "PhD candidate at Tilburg University in the Department of Methodology and Statistics"
  },
  {
    "objectID": "presenters/mihai-constantin.html#section",
    "href": "presenters/mihai-constantin.html#section",
    "title": "Mihai Constantin",
    "section": "",
    "text": "PhD candidate at Tilburg University in the Department of Methodology and Statistics"
  },
  {
    "objectID": "presenters/mihai-constantin.html#interests",
    "href": "presenters/mihai-constantin.html#interests",
    "title": "Mihai Constantin",
    "section": "üîç Interests",
    "text": "üîç Interests\nMingling psychology with data science and software engineering\nComputational statistics and developing software tools for social sciences\nSimulation-based methods for sample size analysis"
  },
  {
    "objectID": "exercises/power-analysis-var-1.html",
    "href": "exercises/power-analysis-var-1.html",
    "title": "Simulation-based power analysis for the \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models",
    "section": "",
    "text": "The code below chunk simply makes sure that all the libraries used here are installed. We should first check if the R packages are installed before we proceed.\n\n# Do not run because we do not want to install packages (this should be your decision)\n\nlist.of.packages = c(\"data.table\",\"psych\",\"ggplot2\",\"tidyverse\",\"MASS\")\nnew.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\n\nNow that we have all packages installed, we continue by loading them.\n\nlibrary(data.table) # to create lagged outcome\nlibrary(psych) # to compute descriptive statistics\nlibrary(ggplot2) # for making plots\nlibrary(tidyverse) # a useful package\n\nlibrary(MASS)\n\nset.seed(1235) # Set a seed to reproduce analyses"
  },
  {
    "objectID": "exercises/power-analysis-var-1.html#setup-the-environment",
    "href": "exercises/power-analysis-var-1.html#setup-the-environment",
    "title": "Simulation-based power analysis for the \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models",
    "section": "",
    "text": "The code below chunk simply makes sure that all the libraries used here are installed. We should first check if the R packages are installed before we proceed.\n\n# Do not run because we do not want to install packages (this should be your decision)\n\nlist.of.packages = c(\"data.table\",\"psych\",\"ggplot2\",\"tidyverse\",\"MASS\")\nnew.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\n\nNow that we have all packages installed, we continue by loading them.\n\nlibrary(data.table) # to create lagged outcome\nlibrary(psych) # to compute descriptive statistics\nlibrary(ggplot2) # for making plots\nlibrary(tidyverse) # a useful package\n\nlibrary(MASS)\n\nset.seed(1235) # Set a seed to reproduce analyses"
  },
  {
    "objectID": "exercises/power-analysis-var-1.html#preparing-the-functions",
    "href": "exercises/power-analysis-var-1.html#preparing-the-functions",
    "title": "Simulation-based power analysis for the \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models",
    "section": "Preparing the functions",
    "text": "Preparing the functions\nWe need two functions to conduct a simulation-based power analysis. The first one generates the datasets, and the second run the Monte-Carlo simulation to estimate the empirical power.\n\nFunction 1: Data generation for the \\(\\text{VAR}(1)\\) model\nThis function generates a dataset from an \\(\\text{VAR}(1)\\) model where ‚Äòvars‚Äô is the number of variables of the \\(\\text{VAR}(1)\\) model, ‚ÄòTobs‚Äô is the number of repeated measurements, ‚Äòdelta‚Äô the intercept matrix, ‚Äòpsi‚Äô the transition matrix (which contains the auto- and cross-regressive effects), and ‚Äòsigma‚Äô the variance-covariance matrix of the innovation.\n\n# Function to generate data from an $\\text{AR}(1)$ Model\nsim_VAR_data = function(vars,Tobs,delta,psi,sigma){\n\n  # Create number of observations: N + T.burning\n  T.burning = 10000 # Number of burning observations\n  T.total = T.burning + Tobs\n\n  # Simulate errors\n  if (vars == 1){\n    E = as.matrix(rnorm(T.total, 0, sigma))\n  } else {\n    E = mvrnorm(T.total, mu=rep(0,vars), sigma)\n  }\n\n  # Recursive equation\n  Y = matrix(0,T.total,vars)\n\n  # Initialized values\n  Y[1,] = delta + E[1,]\n\n  # Simulate Dependent Variables\n  for (t in 2:T.total){\n      Y[t,] = delta + psi%*%Y[t-1,] + E[t,]\n  }\n\n  # Exclude burning observations, create lag variable and rename columns\n  Y = Y[-seq(1:T.burning),]\n  Y = cbind(Y,lag(Y))\n  colnames(Y) = c(sprintf(\"Y%d\",seq(1:vars)),sprintf(\"Y%dlag\",seq(1:vars)))\n\n  return(as.data.frame(Y))\n}\n\n\n\nFunction 2: Conduct the Monte Carlo simulation\nThis function conducts the Monte Carlo simulation for a set of ‚ÄòTobs‚Äô and computed statistical power for a given hypothesis. The arguments of the function are: ‚Äòvars‚Äô is the number of variables of the \\(\\text{VAR}(1)\\) model, ‚ÄòTobs_list‚Äô is a list of numbers of repeated measurements (‚ÄòTobs‚Äô), ‚Äòdelta‚Äô the intercept matrix, ‚Äòpsi‚Äô the transition matrix (which contains the auto- and cross-regressive effects), and ‚Äòsigma‚Äô the variance-covariance matrix of the innovation, ‚ÄòR‚Äô is the number of Monte Carlo replicates (e.g., 1000), ‚Äòalpha‚Äô is the Type I error rate (or significance level of a test).\n\n# Function to conduct the Monte Carlo simulation and compute statistical power for a given Tobs\n\nmc_power = function(vars,Tobs_list,delta,psi,sigma,R,alpha){\n  # Generate dataset\n  df_pow = data.frame()\n\n  # Loop over the sample size list\n  for (i in 1:length(Tobs_list)){\n    Tobs = Tobs_list[i]\n    print(paste0(\"Power analysis for N = \", Tobs))\n\n    # R replicates for each sample size\n    for (r in 1:R){\n\n      # Generate VAR data\n      data = sim_VAR_data(vars,Tobs,delta,psi,sigma)\n\n      # Create names list\n      var_names = sprintf(\"Y%d\",seq(1:vars))\n      lag_names = sprintf(\"Y%dlag\",seq(1:vars))\n\n      # Estimate models\n      list_pow_rep = list()\n\n      for (nbName in 1:length(var_names)){\n        name_ = var_names[nbName]\n\n        # Create formula\n        formula = as.formula(paste(name_ ,paste(lag_names, collapse = \" + \"), sep = \" ~ \"))\n\n        # Estimate model\n        model = lm(formula, data)\n\n        # Extract coefs\n        sum = summary(model)$coefficients\n\n        # Extract p.values\n        df_pval = as.data.frame(rbind(sum[,4]))\n\n        # Compute power\n        list_pow_rep[[nbName]] =  as.data.frame(df_pval &lt; alpha)\n        names(list_pow_rep[[nbName]]) = c(paste0(\"pow_int_\",name_), paste0(\"pow_\",lag_names,\"_\",name_))\n      }\n\n      rep_data = bind_cols(data.frame(Tobs = Tobs), do.call(cbind, list_pow_rep))\n      df_pow = rbind(df_pow, rep_data)\n    }\n  }\n\n  # Compute power\n  df_pow = aggregate(df_pow[, 2:ncol(df_pow)], list(df_pow$Tobs), mean)\n  df_pow = rename (df_pow, Tobs = Group.1)\n\n  return(df_pow)\n}"
  },
  {
    "objectID": "exercises/power-analysis-var-1.html#conduct-the-simulation-based-power-analysis",
    "href": "exercises/power-analysis-var-1.html#conduct-the-simulation-based-power-analysis",
    "title": "Simulation-based power analysis for the \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models",
    "section": "Conduct the simulation-based power analysis",
    "text": "Conduct the simulation-based power analysis\nTo conduct the simulation-based power analysis you first need to set the following arguments:\n\n‚ÄòTobs‚Äô: list of numbers of repeated measurements.\n‚Äòvars‚Äô: number of variables of the \\(\\text{VAR}(1)\\) model.\n‚Äòdelta‚Äô: a vars*1 matrix with one intercept per variable.\n‚Äòpsi‚Äô: a vars*vars matrix. The diagonal elements are the autoregressive coefficients, and the off-diagonal are the cross-regressive coefficients.\n‚Äòsigma‚Äô: a var*vars matrix. The diagonal elements are the variance of the residuals, and the off-diagonal elements are the covariance values. Note that the matrix should be symmetric.\n‚Äòalpha‚Äô: type I error rate (often .05).\n‚ÄòR‚Äô: number of Monte Carlo replicates (often 1000).\n‚Äòpow_target‚Äô: the empirical power targeted, which is often .8. This variable is only used in the results (not in the simulation).\n\nWe demonstrate how to run this analysis with two examples: an \\(\\text{AR}(1)\\) and a \\(\\text{VAR}(1)\\) model.\n\nExample of a \\(\\text{AR}(1)\\) model\nWe ran a \\(\\text{AR}(1)\\) model with Tobs = 50, 100 and 150 and for a \\(\\text{AR}(1)\\) model as follows:\n\\[\ny_{t} = 3 + .3 * y_{t-1} + \\varepsilon\n\\]\nwith:\n\\[\n\\varepsilon \\sim N(0, 10)\n\\]\n\n# Set the values for conducting the power analysis\nTobs_list = c(50,100,150)\nvars = 1\ndelta = as.matrix(3)\npsi = as.matrix(.3)\nsigma = as.matrix(10)\nalpha = 0.05\nR = 10\n\npow_target = .8\n\n# Conduct the power analysis\ndf_pow_result = mc_power(vars,Tobs_list,delta,psi,sigma,R,alpha)\n\n[1] \"Power analysis for N = 50\"\n[1] \"Power analysis for N = 100\"\n[1] \"Power analysis for N = 150\"\n\n\nFinally, we display the results of the power analysis in a plot and a recap table:\n\n# Plot the results of the power analysis\ndt = df_pow_result %&gt;%\n        gather(Coef_power, power, starts_with(\"pow\"))\ndt$Coef_power = stringr::str_replace_all(dt$Coef_power, c(\"pow_\"=\"\", \"_\"=\" -&gt; \")) # rename\nggplot() +\n        geom_line(data=dt, aes(y=power,x=Tobs, color=Coef_power)) +\n        geom_hline(yintercept = pow_target, color=\"red\", linetype=2) +\n        scale_y_continuous(breaks=seq(0,1,by=.1), limits=c(0,1)) +\n        labs(y=\"Power\", x=\"Time points\") +\n        theme_classic()\n\n\n\n\n\n\n\n\n\n# Create table of outputs\ndt = df_pow_result %&gt;%\n        gather(pow,value,starts_with(\"pow\")) %&gt;%\n        spread(Tobs, value)\ndt$pow = stringr::str_replace_all(dt$pow, c(\"pow_\"=\"\", \"_\"=\" -&gt; \")) # rename\nnames(dt)[1] = \"Coefficients\"\ndt\n\n  Coefficients  50 100 150\n1    int -&gt; Y1 0.4 0.9 1.0\n2  Y1lag -&gt; Y1 0.5 1.0 0.9\n\n\n\n\nExample of a \\(\\text{VAR}(1)\\) model with 3 variables\nWe run a power analysis with Tobs = 50 and 100 and for a \\(\\text{VAR}(1)\\) model with 3 variables as follows:\n\\[\n    \\begin{bmatrix} y_{1t} \\\\ y_{2t} \\\\ y_{3t} \\end{bmatrix} =\n    \\begin{bmatrix} 4 \\\\ 6 \\\\ 10 \\end{bmatrix} +\n    \\begin{bmatrix} 0.5 \\ 0.14 \\ .2 \\\\ 0.1 \\ 0.4 \\ .03 \\\\ 0.05 \\ 0.12 \\ .6 \\end{bmatrix}\n    \\begin{bmatrix} y_{1, t-1} \\\\ y_{2, t-1} \\\\ y_{3, t-1} \\end{bmatrix} +\n    \\begin{bmatrix} \\varepsilon_{1t} \\\\ \\varepsilon_{2t} \\\\ \\varepsilon_{3t} \\end{bmatrix}\n\\]\nwith:\n\\[\n    \\begin{bmatrix} \\varepsilon_{1t} \\\\ \\varepsilon_{2t} \\\\ \\varepsilon_{3t} \\end{bmatrix} \\sim\n    N \\Bigg( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} , \\begin{bmatrix} 15 \\ 3 \\ 6 \\\\ 3 \\ 20 \\ 9 \\\\ 6 \\ 9 \\ 18  \\end{bmatrix} \\Bigg)\n\\]\n\n# Set the values for conducting the power analysis\nTobs_list = c(50, 100, 150)\nvars = 3\ndelta = as.matrix(c(4,6,10))\npsi = as.matrix(rbind(c(.5, .14, .2),\n                      c(.1, .4, .03),\n                      c(.05, .12, .5)))\nsigma = as.matrix(rbind(c(15, 3, 6),\n                        c(3, 20, 9),\n                        c(6, 9, 18)))\nalpha = 0.05\nR = 10\n\npow_target = .8\n\n# Conduct the power analysis\ndf_pow_result = mc_power(vars,Tobs_list,delta,psi,sigma,R,alpha)\n\n[1] \"Power analysis for N = 50\"\n[1] \"Power analysis for N = 100\"\n[1] \"Power analysis for N = 150\"\n\n\nFinally, we display the results of the power analysis in a plot and a recap table:\n\n# Plot the results of the power analysis\ndt = df_pow_result %&gt;%\n        gather(Coef_power, power, starts_with(\"pow\"))\ndt$Coef_power = stringr::str_replace_all(dt$Coef_power, c(\"pow_\"=\"\", \"_\"=\" -&gt; \")) # rename\nggplot() +\n        geom_line(data=dt, aes(y=power,x=Tobs, color=Coef_power)) +\n        geom_hline(yintercept = pow_target, color=\"red\", linetype=2) +\n        scale_y_continuous(breaks=seq(0,1,by=.1), limits=c(0,1)) +\n        labs(y=\"Power\", x=\"Time points\") +\n        theme_classic()\n\n\n\n\n\n\n\n\n\n# Create table of outputs\ndt = df_pow_result %&gt;%\n        gather(pow,value,starts_with(\"pow\")) %&gt;%\n        spread(Tobs, value)\ndt$pow = stringr::str_replace_all(dt$pow, c(\"pow_\"=\"\", \"_\"=\" -&gt; \")) # rename\nnames(dt)[1] = \"Coefficients\"\ndt\n\n   Coefficients  50 100 150\n1     int -&gt; Y1 0.5 0.7 0.9\n2     int -&gt; Y2 0.4 0.6 1.0\n3     int -&gt; Y3 1.0 1.0 1.0\n4   Y1lag -&gt; Y1 1.0 1.0 1.0\n5   Y1lag -&gt; Y2 0.3 0.1 0.1\n6   Y1lag -&gt; Y3 0.0 0.0 0.1\n7   Y2lag -&gt; Y1 0.0 0.2 0.5\n8   Y2lag -&gt; Y2 0.5 0.9 1.0\n9   Y2lag -&gt; Y3 0.0 0.1 0.3\n10  Y3lag -&gt; Y1 0.2 0.4 0.8\n11  Y3lag -&gt; Y2 0.1 0.1 0.0\n12  Y3lag -&gt; Y3 0.9 1.0 1.0"
  },
  {
    "objectID": "exercises/power-analysis-var-1.html#get-the-session-info",
    "href": "exercises/power-analysis-var-1.html#get-the-session-info",
    "title": "Simulation-based power analysis for the \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models",
    "section": "Get the session info",
    "text": "Get the session info\nBelow we provide the session information (i.e., operating system, details about the R installation, and so on) for reproducibility purposes.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-58.4     lubridate_1.9.2   forcats_1.0.0     stringr_1.5.0    \n [5] dplyr_1.1.2       purrr_1.0.1       readr_2.1.4       tidyr_1.3.0      \n [9] tibble_3.2.1      tidyverse_2.0.0   ggplot2_3.4.2     psych_2.3.3      \n[13] data.table_1.14.8\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      jsonlite_1.8.4    compiler_4.3.0    tidyselect_1.2.0 \n [5] parallel_4.3.0    scales_1.2.1      yaml_2.3.7        fastmap_1.1.1    \n [9] lattice_0.21-8    R6_2.5.1          labeling_0.4.2    generics_0.1.3   \n[13] knitr_1.43        htmlwidgets_1.6.2 munsell_0.5.0     pillar_1.9.0     \n[17] tzdb_0.4.0        rlang_1.1.1       utf8_1.2.3        stringi_1.7.12   \n[21] xfun_0.39         timechange_0.2.0  cli_3.6.1         withr_2.5.0      \n[25] magrittr_2.0.3    digest_0.6.31     grid_4.3.0        rstudioapi_0.14  \n[29] hms_1.1.3         lifecycle_1.0.3   nlme_3.1-162      vctrs_0.6.2      \n[33] mnormt_2.1.1      evaluate_0.21     glue_1.6.2        farver_2.1.1     \n[37] fansi_1.0.4       colorspace_2.1-0  rmarkdown_2.22    tools_4.3.0      \n[41] pkgconfig_2.0.3   htmltools_0.5.5"
  },
  {
    "objectID": "exercises/sample-size-analysis-ggm.html",
    "href": "exercises/sample-size-analysis-ggm.html",
    "title": "Sample Size Help",
    "section": "",
    "text": "To be added by Mihai, just for reference."
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html",
    "href": "exercises/sample-size-solutions-n-1.html",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "",
    "text": "The code below chunk simply makes sure that all the libraries used here are installed. We should first check if the R packages are installed before we proceed.\n\n## Do not run because we do not want to install packages (this should be your decision)\n\nlist.of.packages = c(\"data.table\",\"psych\",\"ggplot2\",\"tidyverse\",\"MASS\")\nnew.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\n\nNow that we have all packages installed, we continue by loading them.\n\nlibrary(data.table) # to create lagged outcome\nlibrary(psych) # to compute descriptive statistics\nlibrary(ggplot2) # for making plots\nlibrary(tidyverse) # a useful package\n\nlibrary(MASS)\n\nset.seed(1235) # Set a seed to reproduce analyses"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#setup-the-environment",
    "href": "exercises/sample-size-solutions-n-1.html#setup-the-environment",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "",
    "text": "The code below chunk simply makes sure that all the libraries used here are installed. We should first check if the R packages are installed before we proceed.\n\n## Do not run because we do not want to install packages (this should be your decision)\n\nlist.of.packages = c(\"data.table\",\"psych\",\"ggplot2\",\"tidyverse\",\"MASS\")\nnew.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\n\nNow that we have all packages installed, we continue by loading them.\n\nlibrary(data.table) # to create lagged outcome\nlibrary(psych) # to compute descriptive statistics\nlibrary(ggplot2) # for making plots\nlibrary(tidyverse) # a useful package\n\nlibrary(MASS)\n\nset.seed(1235) # Set a seed to reproduce analyses"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#the-leuven-clinical-study-data-set",
    "href": "exercises/sample-size-solutions-n-1.html#the-leuven-clinical-study-data-set",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "The Leuven clinical study data set",
    "text": "The Leuven clinical study data set\nWe use data from Heininga et al.¬†(2019); this study applies the ESM methodology to study emotion dynamics in people with Major Depressive Disorder. The study consist of an ESM testing period of 7 days in which participants had to fill out questions about mood and social context on their daily lives ten times a day (i.e., 70 measurement occasions). The data set contains 38 participants diagnosed with MDD and 40 control subjects. Participants filled out the ESM questionnaires in a stratified random interval scheme between 9:30 AM and 9:30 PM.\nFirst, we are going to load the data set:\n\n# Load data set\nload(file=\"assets/data/clinical-dataset.RData\")\n\nNow, we are going to explore the data set to get a better understanding of the what‚Äôs inside.\n\n# Select the first participant diagnosed with major depressive\ni.ID = unique(data$PID[data$MDD==1])[1]\n\n# Select data from participant with person identification number PID=101\ndata = data[data$PID==101,]\n\n# Find the dimensions\ndim(data)\n\n[1] 70  8\n\n# Find the structure\nstr(data)\n\n'data.frame':   70 obs. of  8 variables:\n $ PID      : num  101 101 101 101 101 101 101 101 101 101 ...\n $ day      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ daybeep  : num  1 2 3 4 5 6 7 8 9 10 ...\n $ PA       : num  NA 27.3 49.7 43 43 ...\n $ NA.      : num  NA 30.4 23.8 24.2 32.8 19.6 18.4 21.2 23 21.8 ...\n $ anhedonia: num  NA 26 25 25 50 21 42 30 22 30 ...\n $ MDD      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ QIDS     : num  12 12 12 12 12 12 12 12 12 12 ...\n\n# See the first 6 rows\nhead(data)\n\n  PID day daybeep       PA  NA. anhedonia MDD QIDS\n1 101   1       1       NA   NA        NA   1   12\n2 101   1       2 27.33333 30.4        26   1   12\n3 101   1       3 49.66667 23.8        25   1   12\n4 101   1       4 43.00000 24.2        25   1   12\n5 101   1       5 43.00000 32.8        50   1   12\n6 101   1       6 18.00000 19.6        21   1   12\n\n# See the last 6 rows\ntail(data)\n\n   PID day daybeep       PA  NA. anhedonia MDD QIDS\n65 101   7       5 24.33333 31.8        52   1   12\n66 101   7       6 28.66667 20.6        53   1   12\n67 101   7       7 23.33333 23.8        51   1   12\n68 101   7       8 33.66667 36.2        46   1   12\n69 101   7       9 41.66667 21.0        29   1   12\n70 101   7      10 34.00000 18.4        47   1   12\n\n# Find the column names\nnames(data)\n\n[1] \"PID\"       \"day\"       \"daybeep\"   \"PA\"        \"NA.\"       \"anhedonia\"\n[7] \"MDD\"       \"QIDS\"     \n\n# Summary of the data\nsummary(data)\n\n      PID           day       daybeep           PA             NA.       \n Min.   :101   Min.   :1   Min.   : 1.0   Min.   :14.67   Min.   :14.40  \n 1st Qu.:101   1st Qu.:2   1st Qu.: 3.0   1st Qu.:26.08   1st Qu.:21.40  \n Median :101   Median :4   Median : 5.5   Median :33.33   Median :25.90  \n Mean   :101   Mean   :4   Mean   : 5.5   Mean   :33.51   Mean   :29.62  \n 3rd Qu.:101   3rd Qu.:6   3rd Qu.: 8.0   3rd Qu.:41.67   3rd Qu.:36.25  \n Max.   :101   Max.   :7   Max.   :10.0   Max.   :57.33   Max.   :65.60  \n                                          NA's   :6       NA's   :6      \n   anhedonia          MDD         QIDS   \n Min.   :14.00   Min.   :1   Min.   :12  \n 1st Qu.:24.75   1st Qu.:1   1st Qu.:12  \n Median :41.50   Median :1   Median :12  \n Mean   :39.34   Mean   :1   Mean   :12  \n 3rd Qu.:52.00   3rd Qu.:1   3rd Qu.:12  \n Max.   :83.00   Max.   :1   Max.   :12  \n NA's   :6                               \n\n# Number of participants\nlength(unique(data$PID))\n\n[1] 1\n\n\nThe data set contains the following variables: PID that denotes the individual identification number, day is a variable that ranges from 1 to 7 and identifies the day of ESM testing, daybeep is a variable that ranges from 1 to 10 and identifies the number of the prompt or beep within a day. PA is the Positive Affect computed as the mean of items: ‚ÄòHow happy do you feel at the moment?‚Äô, ‚ÄòHow relaxed do you feel at the moment?‚Äô and ‚ÄòHow euphoric do you feel at the moment?‚Äô. NA. is the Negative Affect computed as the mean of items: ‚ÄòHow depressed do you feel at the moment?‚Äô, ‚ÄòHow stressed do you feel at the moment?‚Äô, ‚ÄòHow anxious do you feel at the moment?‚Äô, ‚ÄòHow angry do you feel at the moment?‚Äô and ‚ÄòHow restless do you feel at the moment?‚Äô. anhedonia corresponds to the ESM item ‚ÄòTo what degree do you find it difficult to experience pleasure in activities at the moment?‚Äô. MDD is a dummy variable equal to one when the individual has been diagnosed with MDD and 0 otherwise, finally QIDS denotes the sum of the items of the Quick Inventory of Depressive Symptomatology (i.e.¬†QIDS) [@rush200316]. QIDS was measured before the ESM testing period. Time-varying variables (PA, NA, and anhedonia) have been lagged within days to account for the night breaks."
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#visualizations-and-descriptive-statistics",
    "href": "exercises/sample-size-solutions-n-1.html#visualizations-and-descriptive-statistics",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Visualizations and descriptive statistics",
    "text": "Visualizations and descriptive statistics\nWe first obtain some descriptive statistics including number of observations per day, and compliance.\n\n# Get the number of assessment per day\ntable(data$PID)\n\n\n101 \n 70 \n\n# Compute a binary variable indicating if a participant answered a beep. We take\n# the ESM item PA as reference because in this ESM design participants were not\n# allowed to skip items\ndata$Compliance = ifelse(is.na(data$PA)==FALSE, 1, 0)\n\n# Mean, median of the compliance for the participant PID=101\ndescribe(data$Compliance)\n\n   vars  n mean   sd median trimmed mad min max range skew kurtosis   se\nX1    1 70 0.91 0.28      1       1   0   0   1     1 -2.9     6.48 0.03\n\n\nNext, we can obtain visualizations and statistics of the distribution of the person-level or time-invariant variables variables\n\n# We create a data set that will aggregate the data from the time invariant\n# variables: diagnosis (1 = MDD, 0 = control) and depression (QIDS)\ndt.person = aggregate(cbind(data$MDD,data$QIDS), by = list(data$PID), mean, na.rm = TRUE)\ncolnames(dt.person) = c(\"Group.1\",\"MDD\",\"QIDS\")\ndt.person\n\n  Group.1 MDD QIDS\n1     101   1   12\n\n\nWe now focus on time-varying variables NA, PA, and anhedonia and we obtain visualization and descriptive statistics\n\n# Histogram for the time-varying variable negative affect (NA.)\nggplot(data, aes(NA.)) + geom_histogram(color=\"black\", fill=\"white\",bins=30) + theme_bw()\n\nWarning: Removed 6 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n# Descriptive statistics for NA.\ndescribe(data$NA.)\n\n   vars  n  mean    sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 64 29.62 11.47   25.9   28.13 7.86 14.4 65.6  51.2 1.25     1.01 1.43\n\n# Create obs order variable\ndata$obs = 1:nrow(data)\n\n# Plot the trajectories of the time-varying variable NA by person\ndata %&gt;%\n  ggplot(aes(x = obs, y = NA.)) +\n  geom_point() +\n  geom_line() + theme_bw()\n\nWarning: Removed 6 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n# Histogram for the time-varying variable negative affect (PA)\nggplot(data, aes(PA)) + geom_histogram(color=\"black\", fill=\"white\",bins=30) + theme_bw()\n\nWarning: Removed 6 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n# Descriptive statistics for PA\ndescribe(data$PA)\n\n   vars  n  mean    sd median trimmed   mad   min   max range skew kurtosis\nX1    1 64 33.51 10.32  33.33   33.33 12.35 14.67 57.33 42.67 0.09    -0.84\n     se\nX1 1.29\n\n# Plot the trajectories of the time-varying variable PA by person\ndata %&gt;%\n  ggplot(aes(x = obs, y = PA)) +\n  geom_point() +\n  geom_line() + theme_bw()\n\nWarning: Removed 6 rows containing missing values (`geom_point()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n# Histogram for the time-varying variable anhedonia\nggplot(data, aes(anhedonia)) + geom_histogram(color=\"black\", fill=\"white\",bins=30) + theme_bw()\n\nWarning: Removed 6 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n# Descriptive statistics for anhedonia\ndescribe(data$anhedonia)\n\n   vars  n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 64 39.34 15.84   41.5   38.92 20.02  14  83    69 0.22    -0.77 1.98\n\n# Plot the trajectories of the time-varying variable anhedonia by person\ndata %&gt;%\n  ggplot(aes(x = obs, y = anhedonia)) +\n  geom_point() +\n  geom_line() + theme_bw()\n\nWarning: Removed 6 rows containing missing values (`geom_point()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\nFinally, we create the lagged variables for PA and NA. They will be used on the following \\(\\text{AR}(1)\\) and \\(\\text{VAR}(1)\\) models.\n\n# Create lagged variables: lagged within days to take into account night breaks\ndata$PA.lag = rep(NA,nrow(data))\ndata$NA.lag = rep(NA,nrow(data))\ndata$anhedonia.lag = rep(NA,nrow(data))\nday.id = unique(data$day)\nfor (t in day.id){\n  data$PA.lag[which(data$day==t)] = shift(data$PA[which(data$day==t)],1)\n  data$NA.lag[which(data$day==t)] = shift(data$NA.[which(data$day==t)],1)\n  data$anhedonia.lag[which(data$day==t)] = shift(data$anhedonia[which(data$day==t)],1)\n}"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#estimate-the-textar1-model-for-pa",
    "href": "exercises/sample-size-solutions-n-1.html#estimate-the-textar1-model-for-pa",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "1. Estimate the \\(\\text{AR}(1)\\) model for PA",
    "text": "1. Estimate the \\(\\text{AR}(1)\\) model for PA\nWe estimate an \\(\\text{AR}(1)\\) model for PA using a linear regression model (ordinary least squares, OLS). You can extract the estimates with the ‚Äòsummary()‚Äô function. Finally, you can compute the estimate of the standard deviation of the errors of the \\(\\text{AR}(1)\\) model computing the standard deviation using the function ‚Äòsd()‚Äô on the residuals of the fitted model.\n\n# AR(1) model for PA\nfit.AR.PA = lm(PA ~ 1 + PA.lag, data = data)\nsummary(fit.AR.PA)\n\n\nCall:\nlm(formula = PA ~ 1 + PA.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.486  -5.866   2.070   5.820  18.155 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.3258     4.5755   4.442 4.68e-05 ***\nPA.lag        0.4092     0.1308   3.130  0.00287 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.658 on 52 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1585,    Adjusted R-squared:  0.1423 \nF-statistic: 9.796 on 1 and 52 DF,  p-value: 0.002866\n\n# Estimate the standard deviation of the errors\nsd(residuals(fit.AR.PA))\n\n[1] 9.566865"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#estimate-the-textvar1-model-for-pa-and-na",
    "href": "exercises/sample-size-solutions-n-1.html#estimate-the-textvar1-model-for-pa-and-na",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "2. Estimate the \\(\\text{VAR}(1)\\) model for PA and NA",
    "text": "2. Estimate the \\(\\text{VAR}(1)\\) model for PA and NA\nWe estimate a \\(\\text{VAR}(1)\\) model for PA and NA using two separate linear regression models. You can extract the estimates with the ‚Äòsummary()‚Äô function. Finally, you can compute the estimate of the variance-covariance matrix of the errors of the \\(\\text{VAR}(1)\\) model computing the covariance matrix using the function ‚Äòcov()‚Äô on the residuals of each of the fitted models.\n\n# Linear regression model for PA\nfit.VAR.PA = lm(PA ~ 1 + PA.lag + NA.lag, data = data)\nsummary(fit.VAR.PA)\n\n\nCall:\nlm(formula = PA ~ 1 + PA.lag + NA.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.551  -6.480   1.262   5.859  18.008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.45530    6.27640   3.737 0.000471 ***\nPA.lag       0.39213    0.13341   2.939 0.004930 ** \nNA.lag      -0.08273    0.11299  -0.732 0.467416    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.702 on 51 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1673,    Adjusted R-squared:  0.1346 \nF-statistic: 5.123 on 2 and 51 DF,  p-value: 0.009391\n\n# Linear regression model for NA\nfit.VAR.NA = lm(NA. ~ 1 + PA.lag + NA.lag, data = data)\nsummary(fit.VAR.NA)\n\n\nCall:\nlm(formula = NA. ~ 1 + PA.lag + NA.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.260  -5.933  -2.073   3.104  39.302 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  17.8818     6.6337   2.696  0.00949 **\nPA.lag       -0.0202     0.1410  -0.143  0.88664   \nNA.lag        0.3756     0.1194   3.145  0.00277 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.25 on 51 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1692,    Adjusted R-squared:  0.1366 \nF-statistic: 5.194 on 2 and 51 DF,  p-value: 0.008851\n\n# Estimate variance-covariance matrix of the errors\nres = cbind(residuals(fit.VAR.PA),residuals(fit.VAR.NA))\ncov(res)\n\n          [,1]       [,2]\n[1,] 90.572865   4.295723\n[2,]  4.295723 101.177796"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#estimate-the-textvar1-model-for-na-pa-and-anhedonia",
    "href": "exercises/sample-size-solutions-n-1.html#estimate-the-textvar1-model-for-na-pa-and-anhedonia",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "3. Estimate the \\(\\text{VAR}(1)\\) model for NA, PA and Anhedonia",
    "text": "3. Estimate the \\(\\text{VAR}(1)\\) model for NA, PA and Anhedonia\nWe estimate a \\(\\text{VAR}(1)\\) model for PA, NA and anhedonia using three separate linear regression models. You can extract the estimates with the ‚Äòsummary()‚Äô function. Finally, you can compute the estimate of the variance-covariance matrix of the errors of the \\(\\text{VAR}(1)\\) model by computing the covariance matrix using the function ‚Äòcov()‚Äô on the residuals of each of the fitted models.\n\n# Linear regression model for PA\nfit.VAR.PA = lm(PA ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\nsummary(fit.VAR.PA)\n\n\nCall:\nlm(formula = PA ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.374  -5.907   1.342   5.856  18.787 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   22.60373    6.50921   3.473  0.00107 **\nPA.lag         0.39067    0.13436   2.908  0.00542 **\nNA.lag        -0.12662    0.13926  -0.909  0.36759   \nanhedonia.lag  0.05564    0.10179   0.547  0.58711   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.769 on 50 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1722,    Adjusted R-squared:  0.1226 \nF-statistic: 3.468 on 3 and 50 DF,  p-value: 0.02288\n\n# Linear regression model for NA\nfit.VAR.NA = lm(NA. ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\nsummary(fit.VAR.NA)\n\n\nCall:\nlm(formula = NA. ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.413  -5.302  -2.096   3.864  33.201 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   14.84269    6.66252   2.228   0.0304 *\nPA.lag        -0.02541    0.13752  -0.185   0.8542  \nNA.lag         0.21894    0.14254   1.536   0.1309  \nanhedonia.lag  0.19856    0.10419   1.906   0.0624 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.999 on 50 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.2255,    Adjusted R-squared:  0.179 \nF-statistic: 4.852 on 3 and 50 DF,  p-value: 0.00486\n\n# Linear regression model for anhedonia\nfit.VAR.anhedonia = lm(anhedonia ~ 1 + PA.lag + NA.lag + anhedonia.lag, data = data)\nsummary(fit.VAR.anhedonia)\n\n\nCall:\nlm(formula = anhedonia ~ 1 + PA.lag + NA.lag + anhedonia.lag, \n    data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.750  -9.599   1.195  10.194  29.364 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   14.94851    8.92402   1.675   0.1002  \nPA.lag         0.04386    0.18420   0.238   0.8128  \nNA.lag         0.32640    0.19093   1.710   0.0936 .\nanhedonia.lag  0.30771    0.13955   2.205   0.0321 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.39 on 50 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.268, Adjusted R-squared:  0.2241 \nF-statistic: 6.102 on 3 and 50 DF,  p-value: 0.001276\n\n# Estimate variance-covariance matrix of the errors\nres = cbind(residuals(fit.VAR.PA),residuals(fit.VAR.NA),residuals(fit.VAR.anhedonia))\ncov(res)\n\n          [,1]      [,2]      [,3]\n[1,] 90.034929  2.375884  16.02124\n[2,]  2.375884 94.326082  37.21901\n[3,] 16.021244 37.219012 169.22933"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#how-to-run-the-shiny-app",
    "href": "exercises/sample-size-solutions-n-1.html#how-to-run-the-shiny-app",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "How to run the shiny app?",
    "text": "How to run the shiny app?\nThe shiny application is associated to a package that is stored here: https://gitlab.kuleuven.be/ppw-okpiv/researchers/u0148925/shinyapp-paa_var_n1. To install the package and run the shiny app, please use the following R code in a new script or in a R terminal:\n\n# Install the package\nremotes::install_gitlab(\"ppw-okpiv/researchers/u0148925/shinyapp-paa_var_n1\", host=\"https://gitlab.kuleuven.be\", force=TRUE)\n\n# Import the package in the R session\nlibrary(paavar1)\n\n# Run the shiny app\nrun_paa_var1()"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#exercise-power-analysis-of-textvar1-with-3-variables",
    "href": "exercises/sample-size-solutions-n-1.html#exercise-power-analysis-of-textvar1-with-3-variables",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Exercise: Power analysis of \\(\\text{VAR}(1)\\) with 3 variables",
    "text": "Exercise: Power analysis of \\(\\text{VAR}(1)\\) with 3 variables\n\nPower analysis result\nRunning the simulation with the application, you should end up with a similar plot:\n\n\n\nPower analysis \\(\\text{VAR}(1)\\)\n\n\n\n\nSensitivity analysis for power: varying parameters\nWe slightly changed the values of three coefficients to investigate how they change either the sample size recommendation or the precision of estimates:\n\n\\(\\beta_{11} = .39\\) to \\(\\beta_{11} = .8\\)\n\\(\\sigma_{00} = 90\\) to \\(\\sigma_{00} = 180\\)\n\\(R = 1000\\) to \\(R = 100\\)\n\nWhat conclusions can you draw based on the following power curves?\n\n\n\nSensitivity analysis for power\n\n\n\n\nSensitivity analysis for power: using CI\nFollowing Lafit, Revol et al.¬†(under review), we run a sensitivity analysis using the upper and lower boundaries of the estimated coefficients of interest. First, we extract the 95% confidence interval of the estimated values of each parameter.\n\n# Linear regression model for PA\nconfint(fit.VAR.PA, level=0.95)\n\n                   2.5 %     97.5 %\n(Intercept)    9.5296021 35.6778667\nPA.lag         0.1208077  0.6605412\nNA.lag        -0.4063407  0.1530965\nanhedonia.lag -0.1488169  0.2600881\n\n# Linear regression model for NA\nconfint(fit.VAR.NA, level=0.95)\n\n                    2.5 %     97.5 %\n(Intercept)    1.46062438 28.2247622\nPA.lag        -0.30162831  0.2508175\nNA.lag        -0.06736614  0.5052475\nanhedonia.lag -0.01071022  0.4078257\n\n# Linear regression model for anhedonia\nconfint(fit.VAR.anhedonia, level=0.95)\n\n                    2.5 %     97.5 %\n(Intercept)   -2.97591249 32.8729333\nPA.lag        -0.32611856  0.4138472\nNA.lag        -0.05709339  0.7098859\nanhedonia.lag  0.02741213  0.5880142\n\n\nWe only varied the parameter values for the auto-regressive effect of PA (\\(\\beta_{11}\\)) following the confidence interval. We run two new power analyses. The results are displayed below. What conclusions can you draw based on the following power curves?\n\n\n\nSensitivity analysis for power"
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#exercise-paa-of-textvar1-with-3-variables",
    "href": "exercises/sample-size-solutions-n-1.html#exercise-paa-of-textvar1-with-3-variables",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Exercise: PAA of \\(\\text{VAR}(1)\\) with 3 variables",
    "text": "Exercise: PAA of \\(\\text{VAR}(1)\\) with 3 variables\n\nPAA result\nRunning the simulation with the application, you should end up with a similar plot:\n\n\n\nPAA for \\(\\text{VAR}(1)\\)\n\n\n\n\nSensitivity analysis for power: varying parameters\nWe changed the values of the transition matrix to investigate how it changes the sample size recommendation. What conclusions can you draw based on the following power curves?\n\n\n\nSensitive PAA for \\(\\text{VAR}(1)\\)\n\n\nNote that, despites the raising of the coefficients, the new transition matrix still fulfills the stationary assumption. Higher coefficients could lead to a violation of this assumption."
  },
  {
    "objectID": "exercises/sample-size-solutions-n-1.html#get-the-session-info",
    "href": "exercises/sample-size-solutions-n-1.html#get-the-session-info",
    "title": "Sample size solutions for \\(N = 1\\) intensive longitudinal designs",
    "section": "Get the session info",
    "text": "Get the session info\nBelow we provide the session information (i.e., operating system, details about the R installation, and so on) for reproducibility purposes.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-58.4     lubridate_1.9.2   forcats_1.0.0     stringr_1.5.0    \n [5] dplyr_1.1.2       purrr_1.0.1       readr_2.1.4       tidyr_1.3.0      \n [9] tibble_3.2.1      tidyverse_2.0.0   ggplot2_3.4.2     psych_2.3.3      \n[13] data.table_1.14.8\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      jsonlite_1.8.4    compiler_4.3.0    tidyselect_1.2.0 \n [5] parallel_4.3.0    scales_1.2.1      yaml_2.3.7        fastmap_1.1.1    \n [9] lattice_0.21-8    R6_2.5.1          labeling_0.4.2    generics_0.1.3   \n[13] knitr_1.43        htmlwidgets_1.6.2 munsell_0.5.0     pillar_1.9.0     \n[17] tzdb_0.4.0        rlang_1.1.1       utf8_1.2.3        stringi_1.7.12   \n[21] xfun_0.39         timechange_0.2.0  cli_3.6.1         withr_2.5.0      \n[25] magrittr_2.0.3    digest_0.6.31     grid_4.3.0        rstudioapi_0.14  \n[29] hms_1.1.3         lifecycle_1.0.3   nlme_3.1-162      vctrs_0.6.2      \n[33] mnormt_2.1.1      evaluate_0.21     glue_1.6.2        farver_2.1.1     \n[37] fansi_1.0.4       colorspace_2.1-0  rmarkdown_2.22    tools_4.3.0      \n[41] pkgconfig_2.0.3   htmltools_0.5.5"
  }
]